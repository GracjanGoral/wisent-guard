# Toy example configuration for demonstrating WisentGuard classifier evaluation
# This config provides a quick demonstration of the complete approach using GPT-2

model:
  name: "gpt2"
  layers: [6]  # GPT-2 has 12 layers, use middle layer
  device: "auto"
  half_precision: false
  load_in_8bit: false

# Training data configuration
train_data:
  dataset: "livecodebench/submissions"
  version: "release_v1"
  difficulty: ["easy"]  # Only easy problems for quick demo
  limit: 20  # Very small dataset for demonstration
  
  # Contrastive pair extraction parameters
  min_pass_rate: 0.5  # Minimum pass rate for "correct" code (lowered)
  max_pass_rate: 0.4  # Maximum pass rate for "incorrect" code (raised)
  max_pairs_per_question: 2  # Keep it simple

# Test data configuration
test_data:
  dataset: "livecodebench/submissions"
  version: "release_v2"
  difficulty: ["easy"]  # Only easy problems for quick demo
  limit: 10  # Small test set for demonstration
  
  # Contrastive pair extraction parameters
  min_pass_rate: 0.5
  max_pass_rate: 0.4
  max_pairs_per_question: 2

# Classifier configuration
classifier:
  classifier_type: "logistic"  # Simple and fast
  batch_size: 2  # Small batch for demonstration
  max_pairs: 5  # Limit training pairs for quick demo
  positive_class_label: "bad_code"
  classifier_threshold: 0.5

# Evaluation configuration
evaluation:
  metrics: ["accuracy", "precision", "recall", "f1_score"]
  save_detailed_results: true
  save_confusion_matrix: true
  save_classification_report: true
  
  # Analysis options
  analyze_by_difficulty: false  # Skip for demo
  analyze_confidence_scores: true
  generate_visualizations: false  # Skip for demo

# Output configuration
output:
  results_dir: "demo_results"
  classifier_path: "models/gpt2_toy_classifier.pkl"
  save_metadata: true
  
  # Reproducibility
  save_used_examples: true
  used_examples_path: "demo_results/toy_example_used_examples.json"

# Logging configuration
logging:
  level: "INFO"
  save_log: true
  log_file: "logs/toy_example_gpt2.log"

# Demonstration settings
demo:
  # Fallback to synthetic data if real data fails
  use_synthetic_fallback: true
  synthetic_examples:
    correct_codes:
      - "def add(a, b):\n    return a + b"
      - "def multiply(x, y):\n    return x * y"
      - "def max_value(lst):\n    return max(lst)"
    incorrect_codes:
      - "def add(a, b):\n    return a - b"  # Wrong operation
      - "def multiply(x, y):\n    return x + y"  # Wrong operation
      - "def max_value(lst):\n    return min(lst)"  # Wrong function
  
  # Demo explanation
  description: |
    This configuration demonstrates the complete WisentGuard classifier evaluation approach:
    1. Load ground truth data from LiveCodeBench submissions
    2. Extract correct/incorrect code pairs based on pass@1 scores
    3. Train classifier on activation patterns from these pairs
    4. Evaluate cross-version generalization (v1 â†’ v2)
    5. Measure performance: accuracy, precision, recall, F1-score
    
    The approach answers: How well can activation-based classifiers
    detect code quality issues using real-world submission data?