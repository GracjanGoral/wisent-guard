{
  "metadata": {
    "total_tasks": 6888,
    "created_by": "wisent-guard-task-analyzer",
    "description": "Comprehensive task database with tags and quality scores for lm-evaluation-harness tasks"
  },
  "tasks": {
    "AraDiCE": {
      "tags": [
        "arabic",
        "dialect",
        "multilingual",
        "word-embeddings",
        "semantic-relations"
      ],
      "quality_score": 2
    },
    "AraDiCE_ArabicMMLU_egy": {
      "tags": [
        "history",
        "arabic",
        "general knowledge"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_lev": {
      "tags": [
        "arabic",
        "general knowledge",
        "multilingual"
      ],
      "quality_score": 3
    },
    "aclue": {
      "tags": [
        "chinese",
        "ancient-chinese",
        "classical-literature",
        "reading-comprehension",
        "knowledge"
      ],
      "quality_score": 2
    },
    "aexams": {
      "tags": [
        "academic-exam",
        "standardized-test",
        "multiple-choice",
        "multilingual",
        "knowledge"
      ],
      "quality_score": 2
    },
    "agieval": {
      "tags": [
        "academic-exam",
        "standardized-test",
        "sat",
        "gre",
        "gmat",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "agieval_cn": {
      "tags": [
        "academic-exam",
        "chinese",
        "gaokao",
        "standardized-test",
        "reasoning",
        "knowledge"
      ],
      "quality_score": 3
    },
    "agieval_en": {
      "tags": [
        "academic-exam",
        "english",
        "sat",
        "gre",
        "standardized-test",
        "reasoning",
        "knowledge"
      ],
      "quality_score": 3
    },
    "agieval_nous": {
      "tags": [
        "academic-exam",
        "standardized-test",
        "reasoning",
        "knowledge",
        "english"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_acva": {
      "tags": [
        "arabic",
        "cultural-values",
        "multiple-choice",
        "knowledge",
        "acegpt"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_acva_light": {
      "tags": [
        "arabic",
        "cultural-values",
        "multiple-choice",
        "knowledge",
        "acegpt",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_alghafa": {
      "tags": [
        "arabic",
        "reading-comprehension",
        "sentiment-analysis",
        "question-answering",
        "alghafa"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_alghafa_light": {
      "tags": [
        "arabic",
        "reading-comprehension",
        "sentiment-analysis",
        "question-answering",
        "alghafa",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_exams": {
      "tags": [
        "arabic",
        "academic-exam",
        "standardized-test",
        "knowledge",
        "translation"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_exams_light": {
      "tags": [
        "arabic",
        "academic-exam",
        "standardized-test",
        "knowledge",
        "translation",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "mmlu",
        "translation",
        "academic"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "mmlu",
        "translation",
        "academic",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_arc_challenge": {
      "tags": [
        "arabic",
        "question-answering",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_arc_challenge_light": {
      "tags": [
        "arabic",
        "question-answering",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_arabic_mt_arc_easy": {
      "tags": [
        "arabic",
        "question-answering",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_arc_easy_light": {
      "tags": [
        "arabic",
        "question-answering",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_arabic_mt_boolq": {
      "tags": [
        "arabic",
        "question-answering",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_boolq_light": {
      "tags": [
        "arabic",
        "question-answering",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_arabic_mt_copa": {
      "tags": [
        "arabic",
        "commonsense",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_copa_light": {
      "tags": [
        "arabic",
        "commonsense",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_arabic_mt_hellaswag": {
      "tags": [
        "arabic",
        "commonsense",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_hellaswag_light": {
      "tags": [
        "arabic",
        "commonsense",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_arabic_mt_mmlu": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "translation"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mt_mmlu_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "translation",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_openbook_qa": {
      "tags": [
        "arabic",
        "question-answering",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_openbook_qa_light": {
      "tags": [
        "arabic",
        "question-answering",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_arabic_mt_piqa": {
      "tags": [
        "arabic",
        "question-answering",
        "commonsense",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_piqa_light": {
      "tags": [
        "arabic",
        "question-answering",
        "commonsense",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_arabic_mt_race": {
      "tags": [
        "arabic",
        "reading-comprehension",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_race_light": {
      "tags": [
        "arabic",
        "reading-comprehension",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_arabic_mt_sciq": {
      "tags": [
        "arabic",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_sciq_light": {
      "tags": [
        "arabic",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_arabic_mt_toxigen": {
      "tags": [
        "arabic",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mt_toxigen_light": {
      "tags": [
        "arabic",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_complete": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabicmmlu": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_humanities": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "arabicmmlu_language": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_other": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_social_science": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "arabicmmlu_stem": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "basque_bench": {
      "tags": [
        "basque"
      ],
      "quality_score": 2
    },
    "bbh": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "belebele": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "catalan_bench": {
      "tags": [
        "catalan"
      ],
      "quality_score": 2
    },
    "ceval-valid": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "cmmlu": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "csatqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_at": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_faq": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_gen": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_hs": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ls": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_mc": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_adg_group": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_fic_group": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_group": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_wn_group": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_re": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_sa": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_sum_fp": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_te": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_wic": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "flan_held_in": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "flan_held_out": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "flores_ca": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_es": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_eu": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_gl": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_pt": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "galician_bench": {
      "tags": [
        "galician"
      ],
      "quality_score": 2
    },
    "global_mmlu_ar": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_bn": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_de": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_en": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_es": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_fr": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_humanities": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_other": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_social_sciences": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_stem": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_humanities": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_other": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_social_sciences": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_stem": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_humanities": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_other": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_social_sciences": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_stem": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_humanities": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_other": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_social_sciences": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_stem": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_humanities": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_other": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_social_sciences": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_stem": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_humanities": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_other": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_social_sciences": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_stem": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_humanities": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_other": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_social_sciences": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_stem": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_humanities": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_other": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_social_sciences": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_stem": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_humanities": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_other": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_social_sciences": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_stem": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_humanities": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_other": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_social_sciences": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_stem": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_hi": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_id": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_it": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_ja": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_ko": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_pt": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_sw": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_yo": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_zh": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "haerae": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "hendrycks_math": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "hrm8k": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "hrm8k_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "japanese_leaderboard": {
      "tags": [
        "japanese"
      ],
      "quality_score": 2
    },
    "kmmlu_cot_hard": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_applied_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_humss": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_applied_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_applied_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_humss": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_humss": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_applied_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_humss": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kobest": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "kormedmcqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_bbh": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_gpqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_instruction_following": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_math_hard": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_musr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "lingoly": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_atc": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd10cm": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd10proc": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd9cm": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd9proc": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mela": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "metabench": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_permute": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_secondary": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_secondary_permute": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "minerva_math": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "mmlu": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmmu_val": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_art_and_design": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_business": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_health_and_medicine": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_humanities_and_social_science": {
      "tags": [
        "spanish",
        "stem",
        "humanities",
        "social-science"
      ],
      "quality_score": 2
    },
    "mmmu_val_science": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_tech_and_engineering": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "multimedqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "openllm": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pawsx": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "portuguese_bench": {
      "tags": [
        "portuguese"
      ],
      "quality_score": 2
    },
    "pythia": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "score_non_greedy_robustness_agieval": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "score_non_greedy_robustness_math": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "score_option_order_robustness_agieval": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "score_prompt_robustness_agieval": {
      "tags": [
        "portuguese",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "score_prompt_robustness_math": {
      "tags": [
        "portuguese",
        "stem"
      ],
      "quality_score": 2
    },
    "score_robustness": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "score_robustness_agieval": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "score_robustness_math": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "spanish_bench": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "t0_eval": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tinyBenchmarks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmmluplus": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_STEM": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "wmdp": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xnli": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xwinograd": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "Tag": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_ArabicMMLU_humanities_egy": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_humanities_lev": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_language_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_language_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_other_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_other_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_social-science_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_social-science_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_stem_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_stem_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "advanced_ai_risk": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ai2_arc": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "anli": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arabicmmlu_humanities_tasks": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "arabicmmlu_language_tasks": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_other_tasks": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_social_science_tasks": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "arabicmmlu_stem_tasks": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arc_challenge_mt": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_multilingual": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arithmetic": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "basque-glue": {
      "tags": [
        "basque"
      ],
      "quality_score": 2
    },
    "bertaqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_multiple_choice_a": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_multiple_choice_b": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "cabreu": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "chain_of_thought": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "copal_id": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "eus_exams_es": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "eus_exams_eu": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_at_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_faq_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_hs_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ls_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner-v2_tasks_adg": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner-v2_tasks_fic": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner-v2_tasks_wn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_tasks_adg": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_tasks_fic": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_tasks_wn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_re_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_sa_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_sum_fp-small_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_sum_fp_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_te_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_wic_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "flores": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "freebase": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "french_bench": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_extra": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_gen": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_mc": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_perplexity": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "global_mmlu_full_am_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_humanities_tasks": {
      "tags": [
        "arabic",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_other_tasks": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_social_sciences_tasks": {
      "tags": [
        "arabic",
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_stem_tasks": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_humanities_tasks": {
      "tags": [
        "spanish",
        "bengali",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_other_tasks": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_social_sciences_tasks": {
      "tags": [
        "spanish",
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_stem_tasks": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_humanities_tasks": {
      "tags": [
        "spanish",
        "german",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_other_tasks": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_social_sciences_tasks": {
      "tags": [
        "spanish",
        "german",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_stem_tasks": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_other_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_stem_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_humanities_tasks": {
      "tags": [
        "spanish",
        "french",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_other_tasks": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_social_sciences_tasks": {
      "tags": [
        "spanish",
        "french",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_stem_tasks": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_humanities_tasks": {
      "tags": [
        "spanish",
        "hindi",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_other_tasks": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_social_sciences_tasks": {
      "tags": [
        "spanish",
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_stem_tasks": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_humanities_tasks": {
      "tags": [
        "spanish",
        "italian",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_other_tasks": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_social_sciences_tasks": {
      "tags": [
        "spanish",
        "italian",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_stem_tasks": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_humanities_tasks": {
      "tags": [
        "spanish",
        "japanese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_other_tasks": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_social_sciences_tasks": {
      "tags": [
        "spanish",
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_stem_tasks": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_humanities_tasks": {
      "tags": [
        "spanish",
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_other_tasks": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_social_sciences_tasks": {
      "tags": [
        "spanish",
        "korean",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_stem_tasks": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_humanities_tasks": {
      "tags": [
        "spanish",
        "portuguese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_other_tasks": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_social_sciences_tasks": {
      "tags": [
        "spanish",
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_stem_tasks": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "glue": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "gpt3_translation_benchmarks": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "headqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_multilingual": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hendrycks_ethics": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_mc": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "iwslt2017": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "kbl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_knowledge_em": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "kbl_reasoning_em": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_applied_science_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_humss_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_applied_science_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_applied_science_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_humss_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_humss_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_applied_science_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_humss_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "lambada": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_cloze": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_multilingual": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "llama": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "m_mmlu": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "math_word_problems": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_atc_tasks": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd10cm_tasks": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd10proc_tasks": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd9cm_tasks": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd9proc_tasks": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_arc_subset": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_gsm8k_subset": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_hellaswag_subset": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_mmlu_subset": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "metabench_truthfulqa_subset": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "metabench_winogrande_subset": {
      "tags": [
        "german"
      ],
      "quality_score": 2
    },
    "mgsm_cot_native": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmlu_continuation_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_humanities_generative": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "mmlu_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "mmlu_llama_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "mmlu_llama_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "mmlu_llama_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_other_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_social_sciences_generative": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "mmlu_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "mmlu_stem_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "question-answering"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "question-answering"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_stem_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "paloma": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "phrases_es": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "phrases_va": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "polemo2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "qa4mre": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "qasper": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "score_robustness_mmlu_pro": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "self_consistency": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "storycloze": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "super-glue-lm-eval-v1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "super-glue-lm-eval-v1-seq2seq": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "super-glue-t5-prompt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "sycophancy": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_humanities_tasks": {
      "tags": [
        "spanish",
        "humanities"
      ],
      "quality_score": 2
    },
    "tmlu_other_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_social_sciences_tasks": {
      "tags": [
        "spanish",
        "stem",
        "social-science"
      ],
      "quality_score": 2
    },
    "tmlu_stem_tasks": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_taiwan_specific_tasks": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmmluplus_STEM_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_humanities_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "tmmluplus_other_tasks": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_social_sciences_tasks": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "translation": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "truthfulqa": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_gl": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_multilingual": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_cot": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "unscramble": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmt14": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmt16": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_eu_mt_native": {
      "tags": [
        "basque",
        "translation"
      ],
      "quality_score": 2
    },
    "xquad": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "20_newsgroups": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_ArabicMMLU_high_humanities_history_egy": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_humanities_history_lev": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_humanities_islamic-studies_egy": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_humanities_islamic-studies_lev": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_humanities_philosophy_egy": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_humanities_philosophy_lev": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_language_arabic-language_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_language_arabic-language_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_social-science_civics_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_social-science_civics_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_social-science_economics_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_social-science_economics_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_social-science_geography_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_social-science_geography_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_stem_biology_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_stem_biology_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_stem_computer-science_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_stem_computer-science_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_stem_physics_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_high_stem_physics_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_humanities_history_egy": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_humanities_history_lev": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_humanities_islamic-studies_egy": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_humanities_islamic-studies_lev": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_language_arabic-language_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_language_arabic-language_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_other_general-knowledge_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_other_general-knowledge_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_social-science_civics_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_social-science_civics_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_social-science_economics_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_social-science_economics_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_social-science_geography_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_social-science_geography_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_social-science_social-science_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_social-science_social-science_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_stem_computer-science_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_stem_computer-science_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_stem_natural-science_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_middle_stem_natural-science_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_na_humanities_islamic-studies_egy": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_na_humanities_islamic-studies_lev": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_na_language_arabic-language-general_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_na_language_arabic-language-general_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_na_language_arabic-language-grammar_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_na_language_arabic-language-grammar_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_na_other_driving-test_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "academic-exam"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_na_other_driving-test_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "academic-exam"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_na_other_general-knowledge_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_na_other_general-knowledge_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_humanities_history_egy": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_humanities_history_lev": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_humanities_islamic-studies_egy": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_humanities_islamic-studies_lev": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_language_arabic-language_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_language_arabic-language_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_other_general-knowledge_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_other_general-knowledge_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_social-science_geography_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_social-science_geography_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_social-science_social-science_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_social-science_social-science_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_stem_computer-science_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_stem_computer-science_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_stem_math_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_stem_math_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_stem_natural-science_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_primary_stem_natural-science_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_prof_humanities_law_egy": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_prof_humanities_law_lev": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_univ_other_management_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_univ_other_management_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_univ_social-science_accounting_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_univ_social-science_accounting_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_univ_social-science_economics_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_univ_social-science_economics_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_univ_social-science_political-science_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_univ_social-science_political-science_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_univ_stem_computer-science_egy": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_ArabicMMLU_univ_stem_computer-science_lev": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "AraDiCE_boolq_egy": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_boolq_eng": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_boolq_lev": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_boolq_msa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_egypt_cultural": {
      "tags": [
        "portuguese"
      ],
      "quality_score": 2
    },
    "AraDiCE_jordan_cultural": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_lebanon_cultural": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_openbookqa_egy": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_openbookqa_eng": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_openbookqa_lev": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_openbookqa_msa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_palestine_cultural": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_piqa_egy": {
      "tags": [
        "question-answering",
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_piqa_eng": {
      "tags": [
        "question-answering",
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_piqa_lev": {
      "tags": [
        "question-answering",
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_piqa_msa": {
      "tags": [
        "question-answering",
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_qatar_cultural": {
      "tags": [
        "arabic",
        "question-answering"
      ],
      "quality_score": 2
    },
    "AraDiCE_syria_cultural": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "AraDiCE_truthfulqa_mc1_egy": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "AraDiCE_truthfulqa_mc1_eng": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "AraDiCE_truthfulqa_mc1_lev": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "AraDiCE_truthfulqa_mc1_msa": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "AraDiCE_winogrande_egy": {
      "tags": [
        "german"
      ],
      "quality_score": 2
    },
    "AraDiCE_winogrande_eng": {
      "tags": [
        "german"
      ],
      "quality_score": 2
    },
    "AraDiCE_winogrande_lev": {
      "tags": [
        "german"
      ],
      "quality_score": 2
    },
    "AraDiCE_winogrande_msa": {
      "tags": [
        "german"
      ],
      "quality_score": 2
    },
    "aclue_ancient_chinese_culture": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "aclue_ancient_literature": {
      "tags": [
        "humanities",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_ancient_medical": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_ancient_phonetics": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_basic_ancient_chinese": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "aclue_couplet_prediction": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_homographic_character_resolution": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_named_entity_recognition": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_poetry_appreciate": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_poetry_context_prediction": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_poetry_quality_assessment": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_poetry_sentiment_analysis": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_polysemy_resolution": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_reading_comprehension": {
      "tags": [
        "reading-comprehension",
        "english"
      ],
      "quality_score": 2
    },
    "aclue_sentence_segmentation": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-coordinate-itself": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-coordinate-other-ais": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-coordinate-other-versions": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-corrigible-less-HHH": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-corrigible-more-HHH": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-corrigible-neutral-HHH": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-myopic-reward": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-one-box-tendency": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-power-seeking-inclination": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-self-awareness-general-ai": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-self-awareness-good-text-model": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-self-awareness-text-model": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-self-awareness-training-architecture": {
      "tags": [
        "question-answering",
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-self-awareness-training-web-gpt": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-survival-instinct": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_fewshot-wealth-seeking-inclination": {
      "tags": [
        "few-shot",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-coordinate-itself": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-coordinate-other-ais": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-coordinate-other-versions": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-corrigible-less-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-corrigible-more-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-corrigible-neutral-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-myopic-reward": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-one-box-tendency": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-power-seeking-inclination": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-self-awareness-general-ai": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-self-awareness-good-text-model": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-self-awareness-text-model": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-self-awareness-training-architecture": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-self-awareness-web-gpt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-survival-instinct": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_human-wealth-seeking-inclination": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-coordinate-itself": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-coordinate-other-ais": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-coordinate-other-versions": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-corrigible-less-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-corrigible-more-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-corrigible-neutral-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-myopic-reward": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-one-box-tendency": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-power-seeking-inclination": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-self-awareness-general-ai": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-self-awareness-good-text-model": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-self-awareness-text-model": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-self-awareness-training-architecture": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-self-awareness-training-nn-architecture": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-self-awareness-training-web-gpt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-survival-instinct": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "advanced_ai_risk_lm-wealth-seeking-inclination": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "aexams_Biology": {
      "tags": [
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "aexams_IslamicStudies": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "aexams_Physics": {
      "tags": [
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "aexams_Science": {
      "tags": [
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "aexams_Social": {
      "tags": [
        "social-science",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_amh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_eng": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_ewe": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_fra": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_hau": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_ibo": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_kin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_lin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_lug": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_orm": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_sna": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_sot": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_swa": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_twi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_wol": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_xho": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_yor": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_direct_zul": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_amh": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_eng": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_ewe": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_fra": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_hau": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_ibo": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_kin": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_lin": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_lug": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_orm": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_sna": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_sot": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_swa": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_twi": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_wol": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_xho": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_yor": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_en_cot_zul": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_amh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_eng": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_ewe": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_fra": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_hau": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_ibo": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_kin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_lin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_lug": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_orm": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_sna": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_sot": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_swa": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_twi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_wol": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_xho": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_yor": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimgsm_translate_direct_zul": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrimmlu_direct_amh": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_eng": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_ewe": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_fra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_hau": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_ibo": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_kin": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_lin": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_lug": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_orm": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_sna": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_sot": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_swa": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_twi": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_wol": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_xho": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_yor": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_direct_zul": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_amh": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_eng": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_ewe": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_fra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_hau": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_ibo": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_kin": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_lin": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_lug": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_orm": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_sna": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_sot": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_swa": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_twi": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_wol": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_xho": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_yor": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrimmlu_translate_zul": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "afrixnli_en_direct_amh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_eng": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_ewe": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_fra": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_hau": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_ibo": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_kin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_lin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_lug": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_orm": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_sna": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_sot": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_swa": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_twi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_wol": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_xho": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_yor": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_en_direct_zul": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_amh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_eng": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_ewe": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_fra": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_hau": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_ibo": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_kin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_lin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_lug": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_orm": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_sna": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_sot": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_swa": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_twi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_wol": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_xho": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_yor": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_direct_zul": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_amh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_ewe": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_fra": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_hau": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_ibo": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_kin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_lin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_lug": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_orm": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_sna": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_sot": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_swa": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_twi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_wol": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_xho": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_yor": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_manual_translate_zul": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_amh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_eng": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_ewe": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_fra": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_hau": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_ibo": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_kin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_lin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_lug": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_orm": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_sna": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_sot": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_swa": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_twi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_wol": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_xho": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_yor": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_native_direct_zul": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_amh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_ewe": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_fra": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_hau": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_ibo": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_kin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_lin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_lug": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_orm": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_sna": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_sot": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_swa": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_twi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_wol": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_xho": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_yor": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "afrixnli_translate_zul": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ag_news": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_aqua_rat": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_gaokao_biology": {
      "tags": [
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_gaokao_chemistry": {
      "tags": [
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_gaokao_chinese": {
      "tags": [
        "chinese",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "agieval_gaokao_english": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_gaokao_geography": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_gaokao_history": {
      "tags": [
        "humanities",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_gaokao_mathcloze": {
      "tags": [
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_gaokao_mathqa": {
      "tags": [
        "stem",
        "question-answering",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_gaokao_physics": {
      "tags": [
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_jec_qa_ca": {
      "tags": [
        "question-answering",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_jec_qa_kd": {
      "tags": [
        "question-answering",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_logiqa_en": {
      "tags": [
        "question-answering",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_logiqa_zh": {
      "tags": [
        "question-answering",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_lsat_ar": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_lsat_lr": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_lsat_rc": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_math": {
      "tags": [
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_sat_en": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_sat_en_without_passage": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "agieval_sat_math": {
      "tags": [
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "anagrams1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "anagrams2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "anli_r1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "anli_r2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "anli_r3": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arabic_exams": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "arabic_exams_light": {
      "tags": [
        "arabic",
        "academic-exam",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Algeria": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Algeria_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Ancient_Egypt": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Ancient_Egypt_light": {
      "tags": [
        "arabic",
        "portuguese",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arab_Empire": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arab_Empire_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Architecture": {
      "tags": [
        "arabic",
        "question-answering"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Architecture_light": {
      "tags": [
        "arabic",
        "question-answering",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Art": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Art_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Astronomy": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Astronomy_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Calligraphy": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Calligraphy_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Ceremony": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Ceremony_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Clothing": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Clothing_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Culture": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Culture_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Food": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Food_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Funeral": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Funeral_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Geography": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Geography_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_History": {
      "tags": [
        "arabic",
        "humanities"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_History_light": {
      "tags": [
        "arabic",
        "humanities",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Language_Origin": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Language_Origin_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Literature": {
      "tags": [
        "arabic",
        "humanities"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Literature_light": {
      "tags": [
        "arabic",
        "humanities",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Math": {
      "tags": [
        "arabic",
        "stem"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Math_light": {
      "tags": [
        "arabic",
        "stem",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Medicine": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Medicine_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Music": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Music_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Ornament": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Ornament_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Philosophy": {
      "tags": [
        "arabic",
        "humanities"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Philosophy_light": {
      "tags": [
        "arabic",
        "humanities",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Physics_and_Chemistry": {
      "tags": [
        "arabic",
        "stem"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Physics_and_Chemistry_light": {
      "tags": [
        "arabic",
        "stem",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Arabic_Wedding": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Arabic_Wedding_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Bahrain": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Bahrain_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Comoros": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Comoros_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Egypt_modern": {
      "tags": [
        "arabic",
        "portuguese"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Egypt_modern_light": {
      "tags": [
        "arabic",
        "portuguese",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_InfluenceFromAncientEgypt": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_InfluenceFromAncientEgypt_light": {
      "tags": [
        "arabic",
        "portuguese",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_InfluenceFromByzantium": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_InfluenceFromByzantium_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_InfluenceFromChina": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_InfluenceFromChina_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_InfluenceFromGreece": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_InfluenceFromGreece_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_InfluenceFromIslam": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_InfluenceFromIslam_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_InfluenceFromPersia": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_InfluenceFromPersia_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_InfluenceFromRome": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_InfluenceFromRome_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Iraq": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Iraq_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Islam_Education": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Islam_Education_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Islam_branches_and_schools": {
      "tags": [
        "arabic",
        "spanish"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Islam_branches_and_schools_light": {
      "tags": [
        "arabic",
        "spanish",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Islamic_law_system": {
      "tags": [
        "arabic",
        "stem"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Islamic_law_system_light": {
      "tags": [
        "arabic",
        "stem",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Jordan": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Jordan_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Kuwait": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Kuwait_light": {
      "tags": [
        "arabic",
        "italian",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Lebanon": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Lebanon_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Libya": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Libya_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Mauritania": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Mauritania_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Mesopotamia_civilization": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Mesopotamia_civilization_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Morocco": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Morocco_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Oman": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Oman_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Palestine": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Palestine_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Qatar": {
      "tags": [
        "arabic",
        "question-answering"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Qatar_light": {
      "tags": [
        "arabic",
        "question-answering",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Saudi_Arabia": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Saudi_Arabia_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Somalia": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Somalia_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Sudan": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Sudan_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Syria": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Syria_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Tunisia": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Tunisia_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_United_Arab_Emirates": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_United_Arab_Emirates_light": {
      "tags": [
        "arabic",
        "spanish",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_Yemen": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_Yemen_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_communication": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_communication_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_computer_and_phone": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_computer_and_phone_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_daily_life": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_daily_life_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_acva_entertainment": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_acva_entertainment_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_alghafa_mcq_exams_test_ar": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_alghafa_mcq_exams_test_ar_light": {
      "tags": [
        "arabic",
        "academic-exam",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_alghafa_meta_ar_dialects": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_alghafa_meta_ar_dialects_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_alghafa_meta_ar_msa": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_alghafa_meta_ar_msa_light": {
      "tags": [
        "arabic",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_alghafa_multiple_choice_facts_truefalse_balanced_task": {
      "tags": [
        "arabic",
        "factuality",
        "safety",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_alghafa_multiple_choice_facts_truefalse_balanced_task_light": {
      "tags": [
        "arabic",
        "factuality",
        "safety",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_soqal_task": {
      "tags": [
        "arabic",
        "question-answering",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_soqal_task_light": {
      "tags": [
        "arabic",
        "question-answering",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_xglue_mlqa_task": {
      "tags": [
        "arabic",
        "question-answering",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_xglue_mlqa_task_light": {
      "tags": [
        "arabic",
        "question-answering",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_no_neutral_task": {
      "tags": [
        "arabic",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_no_neutral_task_light": {
      "tags": [
        "arabic",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_task": {
      "tags": [
        "arabic",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_task_light": {
      "tags": [
        "arabic",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_alghafa_multiple_choice_sentiment_task": {
      "tags": [
        "arabic",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_alghafa_multiple_choice_sentiment_task_light": {
      "tags": [
        "arabic",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_leaderboard_arabic_mmlu_abstract_algebra": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_abstract_algebra_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_anatomy": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_anatomy_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_astronomy": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_astronomy_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_business_ethics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_business_ethics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_clinical_knowledge": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_clinical_knowledge_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_college_biology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_college_biology_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_college_chemistry": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_college_chemistry_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_college_computer_science": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_college_computer_science_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_college_mathematics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_college_mathematics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_college_medicine": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_college_medicine_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_college_physics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_college_physics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_computer_security": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_computer_security_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_conceptual_physics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_conceptual_physics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_econometrics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_econometrics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_electrical_engineering": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_electrical_engineering_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_elementary_mathematics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_elementary_mathematics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_formal_logic": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_formal_logic_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "reasoning",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_global_facts": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_global_facts_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_biology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_biology_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_chemistry": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_chemistry_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_computer_science": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_computer_science_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_european_history": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_european_history_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_geography": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_geography_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_government_and_politics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_government_and_politics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_macroeconomics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_macroeconomics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_mathematics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_mathematics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_microeconomics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_microeconomics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_physics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_physics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_psychology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_psychology_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_statistics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_statistics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_us_history": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_us_history_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_high_school_world_history": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_high_school_world_history_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_human_aging": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_human_aging_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_human_sexuality": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_human_sexuality_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_international_law": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_international_law_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_jurisprudence": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_jurisprudence_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_logical_fallacies": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_logical_fallacies_light": {
      "tags": [
        "arabic",
        "spanish",
        "knowledge",
        "multiple-choice",
        "reasoning",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_machine_learning": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_machine_learning_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_management": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_management_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_marketing": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_marketing_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_medical_genetics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_medical_genetics_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_miscellaneous": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_miscellaneous_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_moral_disputes": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_moral_disputes_light": {
      "tags": [
        "arabic",
        "spanish",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_moral_scenarios": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_moral_scenarios_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_nutrition": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_nutrition_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_philosophy": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_philosophy_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_prehistory": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_prehistory_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_professional_accounting": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_professional_accounting_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_professional_law": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_professional_law_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_professional_medicine": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_professional_medicine_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_professional_psychology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_professional_psychology_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_public_relations": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_public_relations_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_security_studies": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_security_studies_light": {
      "tags": [
        "arabic",
        "spanish",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_sociology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_sociology_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_us_foreign_policy": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_us_foreign_policy_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_virology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_virology_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_leaderboard_arabic_mmlu_world_religions": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabic_leaderboard_arabic_mmlu_world_religions_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_mt_arc_challenge": {
      "tags": [
        "arabic",
        "question-answering",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_mt_arc_challenge_light": {
      "tags": [
        "arabic",
        "question-answering",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_mt_arc_easy": {
      "tags": [
        "arabic",
        "question-answering",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_mt_arc_easy_light": {
      "tags": [
        "arabic",
        "question-answering",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_mt_boolq": {
      "tags": [
        "arabic",
        "question-answering",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_mt_boolq_light": {
      "tags": [
        "arabic",
        "question-answering",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_mt_copa": {
      "tags": [
        "arabic",
        "commonsense",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_mt_copa_light": {
      "tags": [
        "arabic",
        "commonsense",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_mt_hellaswag": {
      "tags": [
        "arabic",
        "commonsense",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_mt_hellaswag_light": {
      "tags": [
        "arabic",
        "commonsense",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_mt_mmlu": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "translation"
      ],
      "quality_score": 3
    },
    "arabic_mt_mmlu_light": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "translation",
        "lightweight"
      ],
      "quality_score": 2
    },
    "arabic_mt_openbook_qa": {
      "tags": [
        "arabic",
        "question-answering",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_mt_openbook_qa_light": {
      "tags": [
        "arabic",
        "question-answering",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_mt_piqa": {
      "tags": [
        "arabic",
        "question-answering",
        "commonsense",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_mt_piqa_light": {
      "tags": [
        "arabic",
        "question-answering",
        "commonsense",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_mt_race": {
      "tags": [
        "arabic",
        "reading-comprehension",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_mt_race_light": {
      "tags": [
        "arabic",
        "reading-comprehension",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_mt_sciq": {
      "tags": [
        "arabic",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_mt_sciq_light": {
      "tags": [
        "arabic",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabic_mt_toxigen": {
      "tags": [
        "arabic",
        "translation"
      ],
      "quality_score": 2
    },
    "arabic_mt_toxigen_light": {
      "tags": [
        "arabic",
        "translation",
        "lightweight"
      ],
      "quality_score": 1
    },
    "arabicmmlu_accounting_university": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_arabic_language_general": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_arabic_language_grammar": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_arabic_language_high_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_arabic_language_middle_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_arabic_language_primary_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_biology_high_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabicmmlu_civics_high_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_civics_middle_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_computer_science_high_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabicmmlu_computer_science_middle_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabicmmlu_computer_science_primary_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabicmmlu_computer_science_university": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabicmmlu_driving_test": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "academic-exam"
      ],
      "quality_score": 3
    },
    "arabicmmlu_economics_high_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "arabicmmlu_economics_middle_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "arabicmmlu_economics_university": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "arabicmmlu_general_knowledge": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_general_knowledge_middle_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_general_knowledge_primary_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_geography_high_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_geography_middle_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_geography_primary_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_history_high_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "arabicmmlu_history_middle_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "arabicmmlu_history_primary_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "arabicmmlu_islamic_studies": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_islamic_studies_high_school": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_islamic_studies_middle_school": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_islamic_studies_primary_school": {
      "tags": [
        "arabic",
        "chinese",
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_law_professional": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_management_university": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "arabicmmlu_math_primary_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabicmmlu_natural_science_middle_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabicmmlu_natural_science_primary_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabicmmlu_philosophy_high_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "arabicmmlu_physics_high_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabicmmlu_political_science_university": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "arabicmmlu_social_science_middle_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "arabicmmlu_social_science_primary_school": {
      "tags": [
        "arabic",
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "arc_ar": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_bn": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_ca": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_ca_challenge": {
      "tags": [
        "catalan",
        "question-answering"
      ],
      "quality_score": 2
    },
    "arc_ca_easy": {
      "tags": [
        "catalan",
        "question-answering"
      ],
      "quality_score": 2
    },
    "arc_challenge": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "arc_challenge_chat": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_da": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_de": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_el": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_es": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_fi": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_hu": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_is": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_it": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_nb": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_pl": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_pt": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_challenge_mt_sv": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "arc_da": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_de": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_easy": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_es": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_eu": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_eu_challenge": {
      "tags": [
        "basque",
        "question-answering"
      ],
      "quality_score": 2
    },
    "arc_eu_easy": {
      "tags": [
        "basque",
        "question-answering"
      ],
      "quality_score": 2
    },
    "arc_fr": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_gu": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_hi": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_hr": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_hu": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_hy": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_id": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_it": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_kn": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_ml": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_mr": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_ne": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_nl": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_pt": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_ro": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_ru": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_sk": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_sr": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_sv": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_ta": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_te": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_uk": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_vi": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "arc_zh": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "argument_topic": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arithmetic_1dc": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arithmetic_2da": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arithmetic_2dm": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arithmetic_2ds": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arithmetic_3da": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arithmetic_3ds": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arithmetic_4da": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arithmetic_4ds": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arithmetic_5da": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "arithmetic_5ds": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "asdiv": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "asdiv_cot_llama": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "assin_entailment": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "assin_paraphrase": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "atis": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "babi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "banking77": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bbh_cot_fewshot_boolean_expressions": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_causal_judgement": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_date_understanding": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_disambiguation_qa": {
      "tags": [
        "reasoning",
        "question-answering",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_dyck_languages": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_formal_fallacies": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_geometric_shapes": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_hyperbaton": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_logical_deduction_five_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_logical_deduction_seven_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_logical_deduction_three_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_movie_recommendation": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_multistep_arithmetic_two": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_navigate": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_object_counting": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_penguins_in_a_table": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_reasoning_about_colored_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_ruin_names": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_salient_translation_error_detection": {
      "tags": [
        "reasoning",
        "translation",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_snarks": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_sports_understanding": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_temporal_sequences": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_tracking_shuffled_objects_five_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_tracking_shuffled_objects_seven_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_tracking_shuffled_objects_three_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_web_of_lies": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_fewshot_word_sorting": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_boolean_expressions": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_causal_judgement": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_date_understanding": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_disambiguation_qa": {
      "tags": [
        "reasoning",
        "question-answering",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_dyck_languages": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_formal_fallacies": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_geometric_shapes": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_hyperbaton": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_logical_deduction_five_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_logical_deduction_seven_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_logical_deduction_three_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_movie_recommendation": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_multistep_arithmetic_two": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_navigate": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_object_counting": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_penguins_in_a_table": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_reasoning_about_colored_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_ruin_names": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_salient_translation_error_detection": {
      "tags": [
        "reasoning",
        "translation",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_snarks": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_sports_understanding": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_temporal_sequences": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_tracking_shuffled_objects_five_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_tracking_shuffled_objects_seven_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_tracking_shuffled_objects_three_objects": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_web_of_lies": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_cot_zeroshot_word_sorting": {
      "tags": [
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_boolean_expressions": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_causal_judgement": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_date_understanding": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_disambiguation_qa": {
      "tags": [
        "reasoning",
        "question-answering",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_dyck_languages": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_formal_fallacies": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_geometric_shapes": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_hyperbaton": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_logical_deduction_five_objects": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_logical_deduction_seven_objects": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_logical_deduction_three_objects": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_movie_recommendation": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_multistep_arithmetic_two": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_navigate": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_object_counting": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_penguins_in_a_table": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_reasoning_about_colored_objects": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_ruin_names": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_salient_translation_error_detection": {
      "tags": [
        "reasoning",
        "translation",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_snarks": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_sports_understanding": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_temporal_sequences": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_tracking_shuffled_objects_five_objects": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_tracking_shuffled_objects_seven_objects": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_tracking_shuffled_objects_three_objects": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_web_of_lies": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_fewshot_word_sorting": {
      "tags": [
        "reasoning",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_boolean_expressions": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_causal_judgement": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_date_understanding": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_disambiguation_qa": {
      "tags": [
        "reasoning",
        "question-answering",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_dyck_languages": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_formal_fallacies": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_geometric_shapes": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_hyperbaton": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_logical_deduction_five_objects": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_logical_deduction_seven_objects": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_logical_deduction_three_objects": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_movie_recommendation": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_multistep_arithmetic_two": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_navigate": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_object_counting": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_penguins_in_a_table": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_reasoning_about_colored_objects": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_ruin_names": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_salient_translation_error_detection": {
      "tags": [
        "reasoning",
        "translation",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_snarks": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_sports_understanding": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_temporal_sequences": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_tracking_shuffled_objects_five_objects": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_tracking_shuffled_objects_seven_objects": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_tracking_shuffled_objects_three_objects": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_web_of_lies": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bbh_zeroshot_word_sorting": {
      "tags": [
        "reasoning",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "bec2016eu": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_acm_Arab": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_afr_Latn": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "belebele_als_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_amh_Ethi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_apc_Arab": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_arb_Arab": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_arb_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ars_Arab": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ary_Arab": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_arz_Arab": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_asm_Beng": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_azj_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_bam_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ben_Beng": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ben_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_bod_Tibt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_bul_Cyrl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_cat_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ceb_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ces_Latn": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "belebele_ckb_Arab": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_dan_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_deu_Latn": {
      "tags": [
        "basque"
      ],
      "quality_score": 2
    },
    "belebele_ell_Grek": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_eng_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_est_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_eus_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_fin_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_fra_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_fuv_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_gaz_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_glg_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_grn_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_guj_Gujr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_hat_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_hau_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_heb_Hebr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_hin_Deva": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_hin_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_hrv_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_hun_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_hye_Armn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ibo_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ilo_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ind_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_isl_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ita_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_jav_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_jpn_Jpan": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_kac_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_kan_Knda": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_kat_Geor": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_kaz_Cyrl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_kea_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_khk_Cyrl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_khm_Khmr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_kin_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_kir_Cyrl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_kor_Hang": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_lao_Laoo": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_lin_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_lit_Latn": {
      "tags": [
        "italian"
      ],
      "quality_score": 2
    },
    "belebele_lug_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_luo_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_lvs_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_mal_Mlym": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_mar_Deva": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "belebele_mkd_Cyrl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_mlt_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_mri_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_mya_Mymr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_nld_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_nob_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_npi_Deva": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_npi_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_nso_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_nya_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ory_Orya": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_pan_Guru": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_pbt_Arab": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_pes_Arab": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "belebele_plt_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_pol_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_por_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ron_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_rus_Cyrl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_shn_Mymr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_sin_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_sin_Sinh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_slk_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_slv_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_sna_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_snd_Arab": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_som_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_sot_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_spa_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_srp_Cyrl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ssw_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_sun_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_swe_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_swh_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_tam_Taml": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_tel_Telu": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_tgk_Cyrl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_tgl_Latn": {
      "tags": [
        "galician"
      ],
      "quality_score": 2
    },
    "belebele_tha_Thai": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_tir_Ethi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_tsn_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_tso_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_tur_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_ukr_Cyrl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_urd_Arab": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_urd_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_uzn_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_vie_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_war_Latn": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "belebele_wol_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_xho_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_yor_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_zho_Hans": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_zho_Hant": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_zsm_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "belebele_zul_Latn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_gemma-7b": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_hitz": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_itzuli": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_latxa-13b-v1": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_latxa-13b-v1.1": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_latxa-70b-v1": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_latxa-70b-v1.1": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_latxa-7b-v1": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_latxa-7b-v1.1": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_llama-2-13b": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_llama-2-70b": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_llama-2-7b": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_madlad": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_en_mt_nllb": {
      "tags": [
        "question-answering",
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bertaqa_eu": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bhtc_v2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_abstract_narrative_understanding_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_abstract_narrative_understanding_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_anachronisms_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_anachronisms_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_analogical_similarity_generate_until": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_analogical_similarity_multiple_choice": {
      "tags": [
        "reasoning",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_analytic_entailment_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_analytic_entailment_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_arithmetic_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_arithmetic_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_ascii_word_recognition_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_authorship_verification_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_authorship_verification_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_auto_categorization_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_auto_debugging_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_bbq_lite_json_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_bbq_lite_json_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_bridging_anaphora_resolution_barqa_generate_until": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_causal_judgment_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_causal_judgment_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_cause_and_effect_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_cause_and_effect_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_checkmate_in_one_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_checkmate_in_one_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_chess_state_tracking_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_chinese_remainder_theorem_generate_until": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "bigbench_cifar10_classification_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_cifar10_classification_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_code_line_description_generate_until": {
      "tags": [
        "german"
      ],
      "quality_score": 2
    },
    "bigbench_code_line_description_multiple_choice": {
      "tags": [
        "german",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_codenames_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_color_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_color_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_common_morpheme_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_common_morpheme_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_conceptual_combinations_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_conceptual_combinations_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_conlang_translation_generate_until": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_contextual_parametric_knowledge_conflicts_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_contextual_parametric_knowledge_conflicts_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_crash_blossom_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_crash_blossom_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_crass_ai_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_crass_ai_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_cryobiology_spanish_generate_until": {
      "tags": [
        "spanish",
        "stem"
      ],
      "quality_score": 2
    },
    "bigbench_cryobiology_spanish_multiple_choice": {
      "tags": [
        "spanish",
        "stem",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_cryptonite_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_cs_algorithms_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_cs_algorithms_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_dark_humor_detection_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_dark_humor_detection_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_date_understanding_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_date_understanding_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_disambiguation_qa_generate_until": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_disambiguation_qa_multiple_choice": {
      "tags": [
        "question-answering",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_discourse_marker_prediction_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_discourse_marker_prediction_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_disfl_qa_generate_until": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_dyck_languages_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_dyck_languages_multiple_choice": {
      "tags": [
        "spanish",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_elementary_math_qa_generate_until": {
      "tags": [
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_elementary_math_qa_multiple_choice": {
      "tags": [
        "stem",
        "question-answering",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_emoji_movie_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_emoji_movie_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_emojis_emotion_prediction_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_emojis_emotion_prediction_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_empirical_judgments_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_empirical_judgments_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_english_proverbs_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_english_proverbs_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_english_russian_proverbs_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_english_russian_proverbs_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_entailed_polarity_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_entailed_polarity_hindi_generate_until": {
      "tags": [
        "hindi"
      ],
      "quality_score": 2
    },
    "bigbench_entailed_polarity_hindi_multiple_choice": {
      "tags": [
        "hindi",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_entailed_polarity_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_epistemic_reasoning_generate_until": {
      "tags": [
        "stem",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_epistemic_reasoning_multiple_choice": {
      "tags": [
        "stem",
        "reasoning",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_evaluating_information_essentiality_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_evaluating_information_essentiality_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_fact_checker_generate_until": {
      "tags": [
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_fact_checker_multiple_choice": {
      "tags": [
        "factuality",
        "safety",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_fantasy_reasoning_generate_until": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_fantasy_reasoning_multiple_choice": {
      "tags": [
        "reasoning",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_few_shot_nlg_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_figure_of_speech_detection_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_figure_of_speech_detection_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_formal_fallacies_syllogisms_negation_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_formal_fallacies_syllogisms_negation_multiple_choice": {
      "tags": [
        "spanish",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_gem_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_gender_inclusive_sentences_german_generate_until": {
      "tags": [
        "spanish",
        "german"
      ],
      "quality_score": 2
    },
    "bigbench_general_knowledge_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_general_knowledge_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_geometric_shapes_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_geometric_shapes_multiple_choice": {
      "tags": [
        "spanish",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_goal_step_wikihow_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_goal_step_wikihow_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_gre_reading_comprehension_generate_until": {
      "tags": [
        "reading-comprehension",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_gre_reading_comprehension_multiple_choice": {
      "tags": [
        "reading-comprehension",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_hhh_alignment_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_hhh_alignment_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_hindi_question_answering_generate_until": {
      "tags": [
        "hindi",
        "question-answering"
      ],
      "quality_score": 2
    },
    "bigbench_hindu_knowledge_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_hindu_knowledge_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_hinglish_toxicity_generate_until": {
      "tags": [
        "safety",
        "toxicity",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_hinglish_toxicity_multiple_choice": {
      "tags": [
        "safety",
        "toxicity",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_human_organs_senses_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_human_organs_senses_multiple_choice": {
      "tags": [
        "spanish",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_hyperbaton_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_hyperbaton_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_identify_math_theorems_generate_until": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_identify_math_theorems_multiple_choice": {
      "tags": [
        "stem",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_identify_odd_metaphor_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_identify_odd_metaphor_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_implicatures_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_implicatures_multiple_choice": {
      "tags": [
        "spanish",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_implicit_relations_generate_until": {
      "tags": [
        "italian"
      ],
      "quality_score": 2
    },
    "bigbench_implicit_relations_multiple_choice": {
      "tags": [
        "italian",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_intent_recognition_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_intent_recognition_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_international_phonetic_alphabet_nli_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_international_phonetic_alphabet_nli_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_international_phonetic_alphabet_transliterate_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_intersect_geometry_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_intersect_geometry_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_irony_identification_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_irony_identification_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_kanji_ascii_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_kanji_ascii_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_kannada_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_kannada_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_key_value_maps_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_key_value_maps_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_known_unknowns_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_known_unknowns_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_language_games_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_language_identification_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_language_identification_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_linguistic_mappings_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_linguistics_puzzles_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_list_functions_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_logic_grid_puzzle_generate_until": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_logic_grid_puzzle_multiple_choice": {
      "tags": [
        "reasoning",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_logical_args_generate_until": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_logical_args_multiple_choice": {
      "tags": [
        "reasoning",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_logical_deduction_generate_until": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_logical_deduction_multiple_choice": {
      "tags": [
        "reasoning",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_logical_fallacy_detection_generate_until": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_logical_fallacy_detection_multiple_choice": {
      "tags": [
        "reasoning",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_logical_sequence_generate_until": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_logical_sequence_multiple_choice": {
      "tags": [
        "reasoning",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_mathematical_induction_generate_until": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_mathematical_induction_multiple_choice": {
      "tags": [
        "stem",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_matrixshapes_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_metaphor_boolean_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_metaphor_boolean_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_metaphor_understanding_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_metaphor_understanding_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_minute_mysteries_qa_generate_until": {
      "tags": [
        "spanish",
        "question-answering"
      ],
      "quality_score": 2
    },
    "bigbench_misconceptions_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_misconceptions_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_misconceptions_russian_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_misconceptions_russian_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_mnist_ascii_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_mnist_ascii_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_modified_arithmetic_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_moral_permissibility_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_moral_permissibility_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_movie_dialog_same_or_different_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_movie_dialog_same_or_different_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_movie_recommendation_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_movie_recommendation_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_mult_data_wrangling_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_multiemo_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_multiemo_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_natural_instructions_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_navigate_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_navigate_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_nonsense_words_grammar_generate_until": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "bigbench_nonsense_words_grammar_multiple_choice": {
      "tags": [
        "arabic",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_novel_concepts_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_novel_concepts_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_object_counting_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_odd_one_out_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_odd_one_out_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_operators_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_paragraph_segmentation_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_parsinlu_qa_generate_until": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_parsinlu_qa_multiple_choice": {
      "tags": [
        "question-answering",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_parsinlu_reading_comprehension_generate_until": {
      "tags": [
        "reading-comprehension",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_penguins_in_a_table_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_penguins_in_a_table_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_periodic_elements_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_periodic_elements_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_persian_idioms_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_persian_idioms_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_phrase_relatedness_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_phrase_relatedness_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_physical_intuition_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_physical_intuition_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_physics_generate_until": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_physics_multiple_choice": {
      "tags": [
        "stem",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_physics_questions_generate_until": {
      "tags": [
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_play_dialog_same_or_different_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_play_dialog_same_or_different_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_polish_sequence_labeling_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_presuppositions_as_nli_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_presuppositions_as_nli_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_qa_wikidata_generate_until": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_question_selection_generate_until": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_question_selection_multiple_choice": {
      "tags": [
        "question-answering",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_real_or_fake_text_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_real_or_fake_text_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_reasoning_about_colored_objects_generate_until": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_reasoning_about_colored_objects_multiple_choice": {
      "tags": [
        "reasoning",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_repeat_copy_logic_generate_until": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_rephrase_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_riddle_sense_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_riddle_sense_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_ruin_names_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_ruin_names_multiple_choice": {
      "tags": [
        "spanish",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_salient_translation_error_detection_generate_until": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_salient_translation_error_detection_multiple_choice": {
      "tags": [
        "translation",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_scientific_press_release_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_semantic_parsing_in_context_sparc_generate_until": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_semantic_parsing_spider_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_sentence_ambiguity_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_sentence_ambiguity_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_similarities_abstraction_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_similarities_abstraction_multiple_choice": {
      "tags": [
        "spanish",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_simp_turing_concept_generate_until": {
      "tags": [
        "portuguese"
      ],
      "quality_score": 2
    },
    "bigbench_simple_arithmetic_json_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_simple_arithmetic_json_multiple_choice_generate_until": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_simple_arithmetic_json_subtasks_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_simple_arithmetic_multiple_targets_json_generate_until": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_simple_ethical_questions_generate_until": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_simple_ethical_questions_multiple_choice": {
      "tags": [
        "question-answering",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_simple_text_editing_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_snarks_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_snarks_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_social_iqa_generate_until": {
      "tags": [
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_social_iqa_multiple_choice": {
      "tags": [
        "social-science",
        "question-answering",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_social_support_generate_until": {
      "tags": [
        "social-science",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_social_support_multiple_choice": {
      "tags": [
        "social-science",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_sports_understanding_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_sports_understanding_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_strange_stories_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_strange_stories_multiple_choice": {
      "tags": [
        "spanish",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_strategyqa_generate_until": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_strategyqa_multiple_choice": {
      "tags": [
        "question-answering",
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_sufficient_information_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_suicide_risk_generate_until": {
      "tags": [
        "german"
      ],
      "quality_score": 2
    },
    "bigbench_suicide_risk_multiple_choice": {
      "tags": [
        "german",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_swahili_english_proverbs_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_swahili_english_proverbs_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_swedish_to_german_proverbs_generate_until": {
      "tags": [
        "german"
      ],
      "quality_score": 2
    },
    "bigbench_swedish_to_german_proverbs_multiple_choice": {
      "tags": [
        "german",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_symbol_interpretation_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_symbol_interpretation_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_temporal_sequences_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_temporal_sequences_multiple_choice": {
      "tags": [
        "spanish",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_tense_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_timedial_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_timedial_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_topical_chat_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_tracking_shuffled_objects_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_tracking_shuffled_objects_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_understanding_fables_generate_until": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "bigbench_understanding_fables_multiple_choice": {
      "tags": [
        "spanish",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_undo_permutation_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_undo_permutation_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_unit_conversion_generate_until": {
      "tags": [
        "italian"
      ],
      "quality_score": 2
    },
    "bigbench_unit_conversion_multiple_choice": {
      "tags": [
        "italian",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_unit_interpretation_generate_until": {
      "tags": [
        "italian"
      ],
      "quality_score": 2
    },
    "bigbench_unit_interpretation_multiple_choice": {
      "tags": [
        "italian",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_unnatural_in_context_learning_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_vitaminc_fact_verification_generate_until": {
      "tags": [
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_vitaminc_fact_verification_multiple_choice": {
      "tags": [
        "factuality",
        "safety",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "bigbench_what_is_the_tao_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_what_is_the_tao_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_which_wiki_edit_generate_until": {
      "tags": [
        "italian"
      ],
      "quality_score": 2
    },
    "bigbench_which_wiki_edit_multiple_choice": {
      "tags": [
        "italian",
        "multiple-choice"
      ],
      "quality_score": 2
    },
    "bigbench_winowhy_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_winowhy_multiple_choice": {
      "tags": [
        "multiple-choice",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_word_sorting_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "bigbench_word_unscrambling_generate_until": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_adjunct_island": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_anaphor_gender_agreement": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_anaphor_number_agreement": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_animate_subject_passive": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_animate_subject_trans": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_causative": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_complex_NP_island": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_coordinate_structure_constraint_complex_left_branch": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_coordinate_structure_constraint_object_extraction": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_determiner_noun_agreement_1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_determiner_noun_agreement_2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_determiner_noun_agreement_irregular_1": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_determiner_noun_agreement_irregular_2": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_determiner_noun_agreement_with_adj_2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_determiner_noun_agreement_with_adj_irregular_1": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_determiner_noun_agreement_with_adj_irregular_2": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_determiner_noun_agreement_with_adjective_1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_distractor_agreement_relational_noun": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_distractor_agreement_relative_clause": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_drop_argument": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_ellipsis_n_bar_1": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_ellipsis_n_bar_2": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_existential_there_object_raising": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_existential_there_quantifiers_1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_existential_there_quantifiers_2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_existential_there_subject_raising": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_expletive_it_object_raising": {
      "tags": [
        "italian"
      ],
      "quality_score": 2
    },
    "blimp_inchoative": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_intransitive": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_irregular_past_participle_adjectives": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_irregular_past_participle_verbs": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_irregular_plural_subject_verb_agreement_1": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_irregular_plural_subject_verb_agreement_2": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_left_branch_island_echo_question": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_left_branch_island_simple_question": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_matrix_question_npi_licensor_present": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_npi_present_1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_npi_present_2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_only_npi_licensor_present": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_only_npi_scope": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_passive_1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_passive_2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_principle_A_c_command": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_principle_A_case_1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_principle_A_case_2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_principle_A_domain_1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_principle_A_domain_2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_principle_A_domain_3": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_principle_A_reconstruction": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_regular_plural_subject_verb_agreement_1": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_regular_plural_subject_verb_agreement_2": {
      "tags": [
        "arabic"
      ],
      "quality_score": 2
    },
    "blimp_sentential_negation_npi_licensor_present": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_sentential_negation_npi_scope": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_sentential_subject_island": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_superlative_quantifiers_1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_superlative_quantifiers_2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_tough_vs_raising_1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_tough_vs_raising_2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_transitive": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_wh_island": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_wh_questions_object_gap": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_wh_questions_subject_gap": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_wh_questions_subject_gap_long_distance": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_wh_vs_that_no_gap": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_wh_vs_that_no_gap_long_distance": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_wh_vs_that_with_gap": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "blimp_wh_vs_that_with_gap_long_distance": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "boolq": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "boolq-seq2seq": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "cabreu_abstractive": {
      "tags": [
        "basque"
      ],
      "quality_score": 2
    },
    "cabreu_extractive": {
      "tags": [
        "basque"
      ],
      "quality_score": 2
    },
    "cabreu_extreme": {
      "tags": [
        "basque"
      ],
      "quality_score": 2
    },
    "catalanqa": {
      "tags": [
        "catalan",
        "question-answering"
      ],
      "quality_score": 2
    },
    "catcola": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "cb": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ceval-valid_accountant": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_advanced_mathematics": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_art_studies": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_basic_medicine": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_business_administration": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_chinese_language_and_literature": {
      "tags": [
        "chinese",
        "humanities"
      ],
      "quality_score": 2
    },
    "ceval-valid_civil_servant": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_clinical_medicine": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_college_chemistry": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_college_economics": {
      "tags": [
        "chinese",
        "social-science"
      ],
      "quality_score": 2
    },
    "ceval-valid_college_physics": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_college_programming": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_computer_architecture": {
      "tags": [
        "chinese",
        "question-answering"
      ],
      "quality_score": 2
    },
    "ceval-valid_computer_network": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_discrete_mathematics": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_education_science": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_electrical_engineer": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_environmental_impact_assessment_engineer": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_fire_engineer": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_high_school_biology": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_high_school_chemistry": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_high_school_chinese": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_high_school_geography": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_high_school_history": {
      "tags": [
        "chinese",
        "humanities"
      ],
      "quality_score": 2
    },
    "ceval-valid_high_school_mathematics": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_high_school_physics": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_high_school_politics": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_ideological_and_moral_cultivation": {
      "tags": [
        "chinese",
        "reasoning"
      ],
      "quality_score": 3
    },
    "ceval-valid_law": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_legal_professional": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_logic": {
      "tags": [
        "chinese",
        "reasoning"
      ],
      "quality_score": 3
    },
    "ceval-valid_mao_zedong_thought": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_marxism": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_metrology_engineer": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_middle_school_biology": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_middle_school_chemistry": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_middle_school_geography": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_middle_school_history": {
      "tags": [
        "chinese",
        "humanities"
      ],
      "quality_score": 2
    },
    "ceval-valid_middle_school_mathematics": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_middle_school_physics": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_middle_school_politics": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_modern_chinese_history": {
      "tags": [
        "chinese",
        "humanities"
      ],
      "quality_score": 2
    },
    "ceval-valid_operating_system": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_physician": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_plant_protection": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_probability_and_statistics": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_professional_tour_guide": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_sports_science": {
      "tags": [
        "chinese",
        "stem"
      ],
      "quality_score": 2
    },
    "ceval-valid_tax_accountant": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_teacher_qualification": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_urban_and_rural_planner": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "ceval-valid_veterinary_medicine": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "claim_stance_topic": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "cmmlu_agronomy": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_anatomy": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_ancient_chinese": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_arts": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_astronomy": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_business_ethics": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_chinese_civil_service_exam": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "academic-exam"
      ],
      "quality_score": 3
    },
    "cmmlu_chinese_driving_rule": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_chinese_food_culture": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_chinese_foreign_policy": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_chinese_history": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "cmmlu_chinese_literature": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "cmmlu_chinese_teacher_qualification": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_clinical_knowledge": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_college_actuarial_science": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "cmmlu_college_education": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_college_engineering_hydrology": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_college_law": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_college_mathematics": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "cmmlu_college_medical_statistics": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_college_medicine": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_computer_science": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "cmmlu_computer_security": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_conceptual_physics": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "cmmlu_construction_project_management": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_economics": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "cmmlu_education": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_electrical_engineering": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_elementary_chinese": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_elementary_commonsense": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "commonsense"
      ],
      "quality_score": 3
    },
    "cmmlu_elementary_information_and_technology": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_elementary_mathematics": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "cmmlu_ethnology": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_food_science": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "cmmlu_genetics": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_global_facts": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "cmmlu_high_school_biology": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "cmmlu_high_school_chemistry": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "cmmlu_high_school_geography": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_high_school_mathematics": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "cmmlu_high_school_physics": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "cmmlu_high_school_politics": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_human_sexuality": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_international_law": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_journalism": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_jurisprudence": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_legal_and_moral_basis": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_logical": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "cmmlu_machine_learning": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_management": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_marketing": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_marxist_theory": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_modern_chinese": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_nutrition": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_philosophy": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "cmmlu_professional_accounting": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_professional_law": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_professional_medicine": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_professional_psychology": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "cmmlu_public_relations": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_security_study": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_sociology": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "cmmlu_sports_science": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "cmmlu_traditional_chinese_medicine": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_virology": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cmmlu_world_history": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "cmmlu_world_religions": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "cnn_dailymail": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "cocoteros_es": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "code2text_go": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "code2text_java": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "code2text_javascript": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "code2text_php": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "code2text_python": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "code2text_ruby": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "coedit_gec": {
      "tags": [
        "italian"
      ],
      "quality_score": 2
    },
    "cola": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "commonsense_qa": {
      "tags": [
        "question-answering",
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "copa": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "copa_ar": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "copa_ca": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "copa_es": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "copal_id_colloquial": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "copal_id_standard": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "coqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "coqcat": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_english": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_english_age": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_english_autre": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_english_disability": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_english_gender": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_english_nationality": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_english_physical_appearance": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_english_race_color": {
      "tags": [
        "reading-comprehension",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_english_religion": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_english_sexual_orientation": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_english_socioeconomic": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "crows_pairs_french": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "crows_pairs_french_age": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "crows_pairs_french_autre": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "crows_pairs_french_disability": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "crows_pairs_french_gender": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "crows_pairs_french_nationality": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "crows_pairs_french_physical_appearance": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "crows_pairs_french_race_color": {
      "tags": [
        "french",
        "reading-comprehension"
      ],
      "quality_score": 2
    },
    "crows_pairs_french_religion": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "crows_pairs_french_sexual_orientation": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "crows_pairs_french_socioeconomic": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "csatqa_gr": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "csatqa_li": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "csatqa_rch": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "csatqa_rcs": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "csatqa_rcss": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "csatqa_wr": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "cycle_letters": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "dbpedia_14": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "doc_vqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "drop": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "epec_koref_bin": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "eq_bench": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "escola": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ethics_cm": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ethics_deontology": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ethics_justice": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ethics_utilitarianism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ethics_virtue": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ethos_binary": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "eus_exams_es_ejadministrativo": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_ejauxiliar": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_ejsubalterno": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_ejtecnico": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeayuntamientovitoria": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opebilbao": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeehuadmin": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeehuaux": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeehubiblio": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeehuderecho": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeehueconomicas": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeehuempresariales": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeehusubalterno": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeehutecnico": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeehutecnicob": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeosakiadmin": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeosakiaux": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeosakiauxenf": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeosakicelador": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeosakienf": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeosakijuridico": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeosakioperario": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeosakitecnico": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_opeosakivarios": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_osakidetza1c": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_osakidetza2c": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_osakidetza3c": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_osakidetza4c": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_osakidetza5c": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_osakidetza6c": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_osakidetza7c": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_osakidetza8c": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_es_osakidetza9c": {
      "tags": [
        "spanish",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_ejadministrari": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_ejlaguntza": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_ejlaguntzaile": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_ejteknikari": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opebilbaoeu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeehuadmineu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeehuauxeu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeehubiblioeu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeehuderechoeu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeehueconomicaseu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeehuempresarialeseu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeehusubalternoeu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeehutecnicoeu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeehuteknikarib": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opegasteizkoudala": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeosakiadmineu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeosakiauxenfeu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeosakiauxeu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeosakiceladoreu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeosakienfeu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeosakioperarioeu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeosakitecnicoeu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_opeosakivarioseu": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_osakidetza1e": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_osakidetza2e": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_osakidetza3e": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_osakidetza5e": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_osakidetza6e": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_exams_eu_osakidetza7e": {
      "tags": [
        "basque",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "eus_proficiency": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "eus_reading": {
      "tags": [
        "reading-comprehension",
        "english"
      ],
      "quality_score": 2
    },
    "eus_trivia": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_at_prompt-1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_at_prompt-2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_at_prompt-3": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_at_prompt-4": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_at_prompt-5": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_at_prompt-6": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_faq_prompt-1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_faq_prompt-2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_faq_prompt-3": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_faq_prompt-4": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_faq_prompt-5": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_faq_prompt-6": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_hs_prompt-1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_hs_prompt-2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_hs_prompt-3": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_hs_prompt-4": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_hs_prompt-5": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_hs_prompt-6": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ls_prompt-1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ls_prompt-2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner-v2_adg_p1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner-v2_adg_p2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner-v2_fic_p1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner-v2_fic_p2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner-v2_wn_p1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner-v2_wn_p2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_adg_p1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_adg_p2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_fic_p1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_fic_p2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_wn_p1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_ner_wn_p2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_re_prompt-1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_re_prompt-2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_sa_prompt-1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_sa_prompt-2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_sa_prompt-3": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_sa_prompt-4": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_sa_prompt-5": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_sa_prompt-6": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_te_prompt-1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_te_prompt-2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_te_prompt-3": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_te_prompt-4": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_te_prompt-5": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_te_prompt-6": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_wic_prompt-1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_wic_prompt-2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_wic_prompt-3": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_wic_prompt-4": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_wic_prompt-5": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-mp_wic_prompt-6": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-sp_sum_task_fp-small_p1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-sp_sum_task_fp-small_p2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-sp_sum_task_fp_p1": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "evalita-sp_sum_task_fp_p2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "fda": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "financial_tweets": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "fld_default": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "fld_logical_formula_default": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "fld_logical_formula_star": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "fld_star": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "flores_ca-de": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_ca-en": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_ca-es": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_ca-eu": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_ca-fr": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_ca-gl": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_ca-it": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_ca-pt": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_de-ca": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_de-es": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_de-eu": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_de-gl": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_de-pt": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_en-ca": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_en-es": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_en-eu": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_en-gl": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_en-pt": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_es-ca": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_es-de": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_es-en": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_es-eu": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_es-fr": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_es-gl": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_es-it": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_es-pt": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_eu-ca": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_eu-de": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_eu-en": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_eu-es": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_eu-fr": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_eu-gl": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_eu-it": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_eu-pt": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_fr-ca": {
      "tags": [
        "spanish",
        "french",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_fr-es": {
      "tags": [
        "spanish",
        "french",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_fr-eu": {
      "tags": [
        "spanish",
        "french",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_fr-gl": {
      "tags": [
        "spanish",
        "french",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_fr-pt": {
      "tags": [
        "spanish",
        "french",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_gl-ca": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_gl-de": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_gl-en": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_gl-es": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_gl-eu": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_gl-fr": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_gl-it": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_gl-pt": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_it-ca": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_it-es": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_it-eu": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_it-gl": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_it-pt": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_pt-ca": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_pt-de": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_pt-en": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_pt-es": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_pt-eu": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_pt-fr": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_pt-gl": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "flores_pt-it": {
      "tags": [
        "spanish",
        "translation"
      ],
      "quality_score": 2
    },
    "french_bench_arc_challenge": {
      "tags": [
        "french",
        "question-answering"
      ],
      "quality_score": 2
    },
    "french_bench_boolqa": {
      "tags": [
        "french",
        "question-answering"
      ],
      "quality_score": 2
    },
    "french_bench_fquadv2": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_fquadv2_bool": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_fquadv2_genq": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_fquadv2_hasAns": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_grammar": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_hellaswag": {
      "tags": [
        "french",
        "commonsense"
      ],
      "quality_score": 2
    },
    "french_bench_multifquad": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_opus_perplexity": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_orangesum_abstract": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_orangesum_title": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_reading_comp": {
      "tags": [
        "french",
        "reading-comprehension"
      ],
      "quality_score": 2
    },
    "french_bench_topic_based_nli": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_trivia": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_vocab": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_wikitext_fr": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "french_bench_xnli": {
      "tags": [
        "french"
      ],
      "quality_score": 2
    },
    "galcola": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "glianorex": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "glianorex_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "glianorex_fr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "global_mmlu_ar_business": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_ar_humanities": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_ar_medical": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_ar_other": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_ar_social_sciences": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_ar_stem": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_bn_business": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_bn_humanities": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_bn_medical": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_bn_other": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_bn_social_sciences": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_bn_stem": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_de_business": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_de_humanities": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_de_medical": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_de_other": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_de_social_sciences": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_de_stem": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_en_business": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_en_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_en_medical": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_en_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_en_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_en_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_es_business": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_es_humanities": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_es_medical": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_es_other": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_es_social_sciences": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_es_stem": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_fr_business": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_fr_humanities": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_fr_medical": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_fr_other": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_fr_social_sciences": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_fr_stem": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_am_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_abstract_algebra": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_anatomy": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_astronomy": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_business_ethics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_clinical_knowledge": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_college_biology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_college_chemistry": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_college_computer_science": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_college_mathematics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_college_medicine": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_college_physics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_computer_security": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_conceptual_physics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_econometrics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_electrical_engineering": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_elementary_mathematics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_formal_logic": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_global_facts": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_biology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_chemistry": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_computer_science": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_european_history": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_geography": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_government_and_politics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_macroeconomics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_mathematics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_microeconomics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_physics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_psychology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_statistics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_us_history": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_high_school_world_history": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_human_aging": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_human_sexuality": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_international_law": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_jurisprudence": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_logical_fallacies": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_machine_learning": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_management": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_marketing": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_medical_genetics": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_miscellaneous": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_moral_disputes": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_moral_scenarios": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_nutrition": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_philosophy": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_prehistory": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_professional_accounting": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_professional_law": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_professional_medicine": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_professional_psychology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_public_relations": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_security_studies": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_sociology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_us_foreign_policy": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_virology": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ar_world_religions": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_abstract_algebra": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_anatomy": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_astronomy": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_business_ethics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_clinical_knowledge": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_college_biology": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_college_chemistry": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_college_computer_science": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_college_mathematics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_college_medicine": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_college_physics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_computer_security": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_conceptual_physics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_econometrics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_electrical_engineering": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_elementary_mathematics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_formal_logic": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_global_facts": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_biology": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_chemistry": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_computer_science": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_european_history": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_geography": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_government_and_politics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_macroeconomics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_mathematics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_microeconomics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_physics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_psychology": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_statistics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_us_history": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_high_school_world_history": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_human_aging": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_human_sexuality": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_international_law": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_jurisprudence": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_logical_fallacies": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_machine_learning": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_management": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_marketing": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_medical_genetics": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_miscellaneous": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_moral_disputes": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_moral_scenarios": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_nutrition": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_philosophy": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_prehistory": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_professional_accounting": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_professional_law": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_professional_medicine": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_professional_psychology": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_public_relations": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_security_studies": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_sociology": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_us_foreign_policy": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_virology": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_bn_world_religions": {
      "tags": [
        "bengali",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_cs_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_abstract_algebra": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_anatomy": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_astronomy": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_business_ethics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_clinical_knowledge": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_college_biology": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_college_chemistry": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_college_computer_science": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_college_mathematics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_college_medicine": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_college_physics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_computer_security": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_conceptual_physics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_econometrics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_electrical_engineering": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_elementary_mathematics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_formal_logic": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_global_facts": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_biology": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_chemistry": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_computer_science": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_european_history": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_geography": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_government_and_politics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_macroeconomics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_mathematics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_microeconomics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_physics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_psychology": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_statistics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_us_history": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_high_school_world_history": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_human_aging": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_human_sexuality": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_international_law": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_jurisprudence": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_logical_fallacies": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_machine_learning": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_management": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_marketing": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_medical_genetics": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_miscellaneous": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_moral_disputes": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_moral_scenarios": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_nutrition": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_philosophy": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_prehistory": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_professional_accounting": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_professional_law": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_professional_medicine": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_professional_psychology": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_public_relations": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_security_studies": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_sociology": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_us_foreign_policy": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_virology": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_de_world_religions": {
      "tags": [
        "german",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_el_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_en_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_abstract_algebra": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_anatomy": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_astronomy": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_business_ethics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_clinical_knowledge": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_college_biology": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_college_chemistry": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_college_computer_science": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_college_mathematics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_college_medicine": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_college_physics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_computer_security": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_conceptual_physics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_econometrics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_electrical_engineering": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_elementary_mathematics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_formal_logic": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_global_facts": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_biology": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_chemistry": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_computer_science": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_european_history": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_geography": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_government_and_politics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_macroeconomics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_mathematics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_microeconomics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_physics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_psychology": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_statistics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_us_history": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_high_school_world_history": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_human_aging": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_human_sexuality": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_international_law": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_jurisprudence": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_logical_fallacies": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_machine_learning": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_management": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_marketing": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_medical_genetics": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_miscellaneous": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_moral_disputes": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_moral_scenarios": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_nutrition": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_philosophy": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_prehistory": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_professional_accounting": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_professional_law": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_professional_medicine": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_professional_psychology": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_public_relations": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_security_studies": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_sociology": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_us_foreign_policy": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_virology": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_es_world_religions": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fa_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fil_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_abstract_algebra": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_anatomy": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_astronomy": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_business_ethics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_clinical_knowledge": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_college_biology": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_college_chemistry": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_college_computer_science": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_college_mathematics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_college_medicine": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_college_physics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_computer_security": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_conceptual_physics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_econometrics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_electrical_engineering": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_elementary_mathematics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_formal_logic": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_global_facts": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_biology": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_chemistry": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_computer_science": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_european_history": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_geography": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_government_and_politics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_macroeconomics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_mathematics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_microeconomics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_physics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_psychology": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_statistics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_us_history": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_high_school_world_history": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_human_aging": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_human_sexuality": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_international_law": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_jurisprudence": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_logical_fallacies": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_machine_learning": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_management": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_marketing": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_medical_genetics": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_miscellaneous": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_moral_disputes": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_moral_scenarios": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_nutrition": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_philosophy": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_prehistory": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_professional_accounting": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_professional_law": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_professional_medicine": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_professional_psychology": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_public_relations": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_security_studies": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_sociology": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_us_foreign_policy": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_virology": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_fr_world_religions": {
      "tags": [
        "french",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ha_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_he_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_abstract_algebra": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_anatomy": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_astronomy": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_business_ethics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_clinical_knowledge": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_college_biology": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_college_chemistry": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_college_computer_science": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_college_mathematics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_college_medicine": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_college_physics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_computer_security": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_conceptual_physics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_econometrics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_electrical_engineering": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_elementary_mathematics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_formal_logic": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_global_facts": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_biology": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_chemistry": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_computer_science": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_european_history": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_geography": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_government_and_politics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_macroeconomics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_mathematics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_microeconomics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_physics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_psychology": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_statistics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_us_history": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_high_school_world_history": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_human_aging": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_human_sexuality": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_international_law": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_jurisprudence": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_logical_fallacies": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_machine_learning": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_management": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_marketing": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_medical_genetics": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_miscellaneous": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_moral_disputes": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_moral_scenarios": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_nutrition": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_philosophy": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_prehistory": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_professional_accounting": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_professional_law": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_professional_medicine": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_professional_psychology": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_public_relations": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_security_studies": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_sociology": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_us_foreign_policy": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_virology": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_hi_world_religions": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_id_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ig_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_abstract_algebra": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_anatomy": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_astronomy": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_business_ethics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_clinical_knowledge": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_college_biology": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_college_chemistry": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_college_computer_science": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_college_mathematics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_college_medicine": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_college_physics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_computer_security": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_conceptual_physics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_econometrics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_electrical_engineering": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_elementary_mathematics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_formal_logic": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_global_facts": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_biology": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_chemistry": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_computer_science": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_european_history": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_geography": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_government_and_politics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_macroeconomics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_mathematics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_microeconomics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_physics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_psychology": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_statistics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_us_history": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_high_school_world_history": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_human_aging": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_human_sexuality": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_international_law": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_jurisprudence": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_logical_fallacies": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_machine_learning": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_management": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_marketing": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_medical_genetics": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_miscellaneous": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_moral_disputes": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_moral_scenarios": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_nutrition": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_philosophy": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_prehistory": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_professional_accounting": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_professional_law": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_professional_medicine": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_professional_psychology": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_public_relations": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_security_studies": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_sociology": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_us_foreign_policy": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_virology": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_it_world_religions": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_abstract_algebra": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_anatomy": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_astronomy": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_business_ethics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_clinical_knowledge": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_college_biology": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_college_chemistry": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_college_computer_science": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_college_mathematics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_college_medicine": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_college_physics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_computer_security": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_conceptual_physics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_econometrics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_electrical_engineering": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_elementary_mathematics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_formal_logic": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_global_facts": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_biology": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_chemistry": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_computer_science": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_european_history": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_geography": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_government_and_politics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_macroeconomics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_mathematics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_microeconomics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_physics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_psychology": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_statistics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_us_history": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_high_school_world_history": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_human_aging": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_human_sexuality": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_international_law": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_jurisprudence": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_logical_fallacies": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_machine_learning": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_management": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_marketing": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_medical_genetics": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_miscellaneous": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_moral_disputes": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_moral_scenarios": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_nutrition": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_philosophy": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_prehistory": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_professional_accounting": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_professional_law": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_professional_medicine": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_professional_psychology": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_public_relations": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_security_studies": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_sociology": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_us_foreign_policy": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_virology": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ja_world_religions": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_abstract_algebra": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_anatomy": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_astronomy": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_business_ethics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_clinical_knowledge": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_college_biology": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_college_chemistry": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_college_computer_science": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_college_mathematics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_college_medicine": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_college_physics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_computer_security": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_conceptual_physics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_econometrics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_electrical_engineering": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_elementary_mathematics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_formal_logic": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_global_facts": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_biology": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_chemistry": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_computer_science": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_european_history": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_geography": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_government_and_politics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_macroeconomics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_mathematics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_microeconomics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_physics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_psychology": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_statistics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_us_history": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_high_school_world_history": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_human_aging": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_human_sexuality": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_international_law": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_jurisprudence": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_logical_fallacies": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_machine_learning": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_management": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_marketing": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_medical_genetics": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_miscellaneous": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_moral_disputes": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_moral_scenarios": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_nutrition": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_philosophy": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_prehistory": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_professional_accounting": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_professional_law": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_professional_medicine": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_professional_psychology": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_public_relations": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_security_studies": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_sociology": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_us_foreign_policy": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_virology": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ko_world_religions": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ky_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_lt_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_mg_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ms_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ne_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_nl_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ny_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pl_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_abstract_algebra": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_anatomy": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_astronomy": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_business_ethics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_clinical_knowledge": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_college_biology": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_college_chemistry": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_college_computer_science": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_college_mathematics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_college_medicine": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_college_physics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_computer_security": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_conceptual_physics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_econometrics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_electrical_engineering": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_elementary_mathematics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_formal_logic": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_global_facts": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_biology": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_chemistry": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_computer_science": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_european_history": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_geography": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_government_and_politics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_macroeconomics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_mathematics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_microeconomics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_physics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_psychology": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_statistics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_us_history": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_high_school_world_history": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_human_aging": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_human_sexuality": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_international_law": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_jurisprudence": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_logical_fallacies": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_machine_learning": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_management": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_marketing": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_medical_genetics": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_miscellaneous": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_moral_disputes": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_moral_scenarios": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_nutrition": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_philosophy": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_prehistory": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_professional_accounting": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_professional_law": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_professional_medicine": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_professional_psychology": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_public_relations": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_security_studies": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_sociology": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_us_foreign_policy": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_virology": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_pt_world_religions": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ro_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_ru_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_si_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sn_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_so_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sr_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sv_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_sw_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_te_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_tr_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_uk_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_vi_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_yo_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_full_zh_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_hi_business": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_hi_humanities": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_hi_medical": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_hi_other": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_hi_social_sciences": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_hi_stem": {
      "tags": [
        "hindi",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_id_business": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_id_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_id_medical": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_id_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_id_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_id_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_it_business": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_it_humanities": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_it_medical": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_it_other": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_it_social_sciences": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_it_stem": {
      "tags": [
        "italian",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_ja_business": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_ja_humanities": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_ja_medical": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_ja_other": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_ja_social_sciences": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_ja_stem": {
      "tags": [
        "japanese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_ko_business": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_ko_humanities": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_ko_medical": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_ko_other": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_ko_social_sciences": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_ko_stem": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_pt_business": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_pt_humanities": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "global_mmlu_pt_medical": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_pt_other": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "global_mmlu_pt_social_sciences": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science"
      ],
      "quality_score": 3
    },
    "global_mmlu_pt_stem": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice",
        "stem"
      ],
      "quality_score": 3
    },
    "global_mmlu_sw_business": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_sw_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_sw_medical": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_sw_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_sw_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_sw_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_yo_business": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_yo_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_yo_medical": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_yo_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_yo_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_yo_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_zh_business": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_zh_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_zh_medical": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_zh_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_zh_social_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "global_mmlu_zh_stem": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "gpqa_diamond_cot_n_shot": {
      "tags": [
        "question-answering",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_diamond_cot_zeroshot": {
      "tags": [
        "question-answering",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_diamond_generative_n_shot": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_diamond_n_shot": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_diamond_zeroshot": {
      "tags": [
        "question-answering",
        "zero-shot",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_extended_cot_n_shot": {
      "tags": [
        "question-answering",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_extended_cot_zeroshot": {
      "tags": [
        "question-answering",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_extended_generative_n_shot": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_extended_n_shot": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_extended_zeroshot": {
      "tags": [
        "question-answering",
        "zero-shot",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_main_cot_n_shot": {
      "tags": [
        "question-answering",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_main_cot_zeroshot": {
      "tags": [
        "question-answering",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_main_generative_n_shot": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_main_n_shot": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "gpqa_main_zeroshot": {
      "tags": [
        "question-answering",
        "zero-shot",
        "english"
      ],
      "quality_score": 2
    },
    "groundcocoa": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "gsm8k": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "gsm8k_cot": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "gsm8k_cot_llama": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "gsm8k_cot_self_consistency": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "gsm8k_cot_zeroshot": {
      "tags": [
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 2
    },
    "gsm_plus": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "gsm_plus_mini": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "haerae_general_knowledge": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "haerae_history": {
      "tags": [
        "humanities",
        "english"
      ],
      "quality_score": 2
    },
    "haerae_loan_word": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "haerae_rare_word": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "haerae_standard_nomenclature": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "headqa_en": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "headqa_es": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 3
    },
    "hellaswag_ar": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_bn": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_ca": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_da": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_de": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_es": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_eu": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_fr": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_gu": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_hi": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_hr": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_hu": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_hy": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_id": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_it": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_kn": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_ml": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_mr": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_ne": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_nl": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_pt": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_ro": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_ru": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_sk": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_sr": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_sv": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_ta": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_te": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_uk": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hellaswag_vi": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "hendrycks_math_algebra": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "hendrycks_math_counting_and_prob": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "hendrycks_math_geometry": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "hendrycks_math_intermediate_algebra": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "hendrycks_math_num_theory": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "hendrycks_math_prealgebra": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "hendrycks_math_precalc": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "histoires_morales": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "hrm8k_gsm8k": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "hrm8k_gsm8k_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "hrm8k_ksm": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "hrm8k_ksm_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "hrm8k_math": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "hrm8k_math_en": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "hrm8k_mmmlu": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "hrm8k_mmmlu_en": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "hrm8k_omni_math": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "hrm8k_omni_math_en": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "humaneval": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "humaneval_64": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "humaneval_plus": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ifeval": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_hindsight_neglect_10shot": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_into_the_unknown": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_memo_trap": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_modus_tollens": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_neqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_pattern_matching_suppression": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_quote_repetition": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_redefine_math": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_repetitive_algebra": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_sig_figs": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "inverse_scaling_winobias_antistereotype": {
      "tags": [
        "safety",
        "toxicity",
        "english"
      ],
      "quality_score": 3
    },
    "iwslt2017-ar-en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "iwslt2017-en-ar": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ja_leaderboard_jaqket_v2": {
      "tags": [
        "japanese"
      ],
      "quality_score": 2
    },
    "ja_leaderboard_jcommonsenseqa": {
      "tags": [
        "japanese",
        "question-answering",
        "commonsense"
      ],
      "quality_score": 2
    },
    "ja_leaderboard_jnli": {
      "tags": [
        "japanese"
      ],
      "quality_score": 2
    },
    "ja_leaderboard_jsquad": {
      "tags": [
        "japanese"
      ],
      "quality_score": 2
    },
    "ja_leaderboard_marc_ja": {
      "tags": [
        "japanese",
        "question-answering"
      ],
      "quality_score": 2
    },
    "ja_leaderboard_mgsm": {
      "tags": [
        "japanese"
      ],
      "quality_score": 2
    },
    "ja_leaderboard_xlsum": {
      "tags": [
        "japanese"
      ],
      "quality_score": 2
    },
    "ja_leaderboard_xwinograd": {
      "tags": [
        "japanese"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2012": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2013": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2014": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2015": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2016": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2017": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2018": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2019": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2020": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2021": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2022": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2023": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_civil_2024": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2012": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2013": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2014": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2015": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2016": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2017": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2018": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2019": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2020": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2021": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2022": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2023": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_criminal_2024": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2012": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2013": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2014": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2015": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2016": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2017": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2018": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2019": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2020": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2021": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2022": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2023": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_public_2024": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2010": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2011": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2012": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2013": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2014": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2015": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2016": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2017": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2018": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2019": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2020": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2021": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2022": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_bar_exam_em_responsibility_2023": {
      "tags": [
        "arabic",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "kbl_case_relevance_qa_p_em": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kbl_case_relevance_qa_q_em": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kbl_causal_reasoning_qa_em": {
      "tags": [
        "reasoning",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "kbl_common_legal_mistake_qa_em": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kbl_common_legal_mistake_qa_reasoning_em": {
      "tags": [
        "reasoning",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "kbl_legal_concept_qa_em": {
      "tags": [
        "portuguese",
        "question-answering"
      ],
      "quality_score": 2
    },
    "kbl_offense_component_qa_em": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kbl_query_and_statute_matching_qa_em": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kbl_statement_consistency_qa_em": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kbl_statute_hallucination_qa_em": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kbl_statute_number_and_content_matching_qa_em": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kmmlu_cot_hard_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_agricultural_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_aviation_engineering_and_maintenance": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_chemical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_civil_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_construction": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_criminal_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_ecology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_economics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_education": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_electronics_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_energy_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_environmental_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_fashion": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_food_processing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_gas_technology_and_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_geomatics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_health": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_industrial_engineer": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_information_technology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_interior_architecture_and_design": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_korean_history": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_machine_design_and_manufacturing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_maritime_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_materials_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_math": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_mechanical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_nondestructive_testing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "academic-exam",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_patent": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_political_science_and_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_public_safety": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_railway_and_automotive_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_real_estate": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_refrigerating_machinery": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_social_welfare": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_taxation": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_cot_hard_telecommunications_and_wireless_technology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_agricultural_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_aviation_engineering_and_maintenance": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_chemical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_civil_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_construction": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_criminal_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_ecology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_economics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_education": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_electronics_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_energy_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_environmental_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_fashion": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_food_processing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_gas_technology_and_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_geomatics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_agricultural_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_aviation_engineering_and_maintenance": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_chemical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_civil_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_construction": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_criminal_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_ecology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_economics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_education": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_electronics_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_energy_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_environmental_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_fashion": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_food_processing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_gas_technology_and_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_geomatics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_health": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_industrial_engineer": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_information_technology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_interior_architecture_and_design": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_korean_history": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_machine_design_and_manufacturing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_maritime_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_materials_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_math": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_mechanical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_nondestructive_testing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "academic-exam",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_patent": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_political_science_and_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_public_safety": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_railway_and_automotive_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_real_estate": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_refrigerating_machinery": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_social_welfare": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_taxation": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_hard_telecommunications_and_wireless_technology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_health": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_industrial_engineer": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_information_technology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_interior_architecture_and_design": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_korean_history": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_machine_design_and_manufacturing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_maritime_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_materials_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_math": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_mechanical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_nondestructive_testing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "academic-exam",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_patent": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_political_science_and_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_public_safety": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_railway_and_automotive_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_real_estate": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_refrigerating_machinery": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_social_welfare": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_taxation": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_direct_telecommunications_and_wireless_technology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_agricultural_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_aviation_engineering_and_maintenance": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_chemical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_civil_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_construction": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_criminal_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_ecology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_economics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_education": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_electronics_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_energy_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_environmental_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_fashion": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_food_processing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_gas_technology_and_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_geomatics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_health": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_industrial_engineer": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_information_technology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_interior_architecture_and_design": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_korean_history": {
      "tags": [
        "korean",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_machine_design_and_manufacturing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_maritime_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_materials_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_math": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_mechanical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_nondestructive_testing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "academic-exam",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_patent": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_political_science_and_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_public_safety": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_railway_and_automotive_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_real_estate": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_refrigerating_machinery": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_social_welfare": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_taxation": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kmmlu_hard_telecommunications_and_wireless_technology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "kobest_boolq": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kobest_copa": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "kobest_hellaswag": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "kobest_sentineg": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "kobest_wic": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "kormedmcqa_dentist": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kormedmcqa_doctor": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kormedmcqa_nurse": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "kormedmcqa_pharm": {
      "tags": [
        "question-answering",
        "safety",
        "toxicity",
        "english"
      ],
      "quality_score": 3
    },
    "lambada_openai": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_cloze_yaml": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_de": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_en": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_es": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_fr": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_it": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_stablelm_de": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_stablelm_en": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_stablelm_es": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_stablelm_fr": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_stablelm_it": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_stablelm_nl": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_openai_mt_stablelm_pt": {
      "tags": [
        "translation",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_standard": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "lambada_standard_cloze_yaml": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "law_stack_exchange": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_bbh_boolean_expressions": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_causal_judgement": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_date_understanding": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_disambiguation_qa": {
      "tags": [
        "reasoning",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_formal_fallacies": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_geometric_shapes": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_hyperbaton": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_logical_deduction_five_objects": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_logical_deduction_seven_objects": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_logical_deduction_three_objects": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_movie_recommendation": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_navigate": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_object_counting": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_penguins_in_a_table": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_reasoning_about_colored_objects": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_ruin_names": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_salient_translation_error_detection": {
      "tags": [
        "reasoning",
        "translation",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_snarks": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_sports_understanding": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_temporal_sequences": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_tracking_shuffled_objects_five_objects": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_tracking_shuffled_objects_seven_objects": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_tracking_shuffled_objects_three_objects": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_bbh_web_of_lies": {
      "tags": [
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_gpqa_diamond": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_gpqa_extended": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_gpqa_main": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_ifeval": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_math_algebra_hard": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_math_counting_and_prob_hard": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_math_geometry_hard": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_math_intermediate_algebra_hard": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_math_num_theory_hard": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_math_prealgebra_hard": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_math_precalculus_hard": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_mmlu_pro": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "leaderboard_musr_murder_mysteries": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_musr_object_placements": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "leaderboard_musr_team_allocation": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "ledgar": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "lingoly_context": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "lingoly_nocontext": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "logieval": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "logiqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "logiqa2": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "m_mmlu_ar": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_bn": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_ca": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_da": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_de": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_en": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_es": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_eu": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_fr": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_gu": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_hi": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_hr": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_hu": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_hy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_id": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_is": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_it": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_kn": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_ml": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_mr": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_nb": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_ne": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_nl": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_pt": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_ro": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_ru": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_sk": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_sr": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_sv": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_ta": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_te": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_uk": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_vi": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "m_mmlu_zh": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mathqa": {
      "tags": [
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mbpp": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mbpp_plus": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mc_taco": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_atc_easy": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_atc_hard": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_atc_medium": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd10cm_easy": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd10cm_hard": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd10cm_medium": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd10proc_easy": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd10proc_hard": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd10proc_medium": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd9cm_easy": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd9cm_hard": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd9cm_medium": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd9proc_easy": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd9proc_hard": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "med_concepts_qa_icd9proc_medium": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "medical_abstracts": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "medmcqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "medqa_4options": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mela_ar": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mela_de": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mela_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mela_es": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mela_fr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mela_is": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mela_it": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mela_ja": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mela_ru": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mela_zh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_arc": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_arc_permute": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_arc_secondary": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_arc_secondary_permute": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_gsm8k": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_gsm8k_secondary": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_hellaswag": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_hellaswag_permute": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_hellaswag_secondary": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_hellaswag_secondary_permute": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_mmlu": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "metabench_mmlu_permute": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "metabench_mmlu_secondary": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "metabench_mmlu_secondary_permute": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "metabench_truthfulqa": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "metabench_truthfulqa_permute": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "metabench_truthfulqa_secondary": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "metabench_truthfulqa_secondary_permute": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "metabench_winogrande": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "metabench_winogrande_permute": {
      "tags": [
        "german"
      ],
      "quality_score": 2
    },
    "metabench_winogrande_secondary": {
      "tags": [
        "german"
      ],
      "quality_score": 2
    },
    "metabench_winogrande_secondary_permute": {
      "tags": [
        "german"
      ],
      "quality_score": 2
    },
    "mgsm_direct_bn": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_ca": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_de": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_es": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_es_spanish_bench": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "mgsm_direct_eu": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_fr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_gl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_ja": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_ru": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_sw": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_te": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_th": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_direct_zh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_en_cot_bn": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_en_cot_de": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_en_cot_en": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_en_cot_es": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_en_cot_fr": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_en_cot_ja": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_en_cot_ru": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_en_cot_sw": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_en_cot_te": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_en_cot_th": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_en_cot_zh": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_bn": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_de": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_en": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_es": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_eu": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_fr": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_ja": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_ru": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_sw": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_te": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_th": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "mgsm_native_cot_zh": {
      "tags": [
        "chain-of-thought",
        "english"
      ],
      "quality_score": 2
    },
    "minerva_math_algebra": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "minerva_math_counting_and_prob": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "minerva_math_geometry": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "minerva_math_intermediate_algebra": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "minerva_math_num_theory": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "minerva_math_prealgebra": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "minerva_math_precalc": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_ar_ar": {
      "tags": [
        "arabic",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_ar_de": {
      "tags": [
        "arabic",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_ar_en": {
      "tags": [
        "arabic",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_ar_es": {
      "tags": [
        "arabic",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_ar_hi": {
      "tags": [
        "arabic",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_ar_vi": {
      "tags": [
        "arabic",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_ar_zh": {
      "tags": [
        "arabic",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_de_ar": {
      "tags": [
        "german",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_de_de": {
      "tags": [
        "german",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_de_en": {
      "tags": [
        "german",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_de_es": {
      "tags": [
        "german",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_de_hi": {
      "tags": [
        "german",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_de_vi": {
      "tags": [
        "german",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_de_zh": {
      "tags": [
        "german",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_en_ar": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_en_de": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_en_en": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_en_es": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_en_hi": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_en_vi": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_en_zh": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_es_ar": {
      "tags": [
        "spanish",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_es_de": {
      "tags": [
        "spanish",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_es_en": {
      "tags": [
        "spanish",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_es_es": {
      "tags": [
        "spanish",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_es_hi": {
      "tags": [
        "spanish",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_es_vi": {
      "tags": [
        "spanish",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_es_zh": {
      "tags": [
        "spanish",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_hi_ar": {
      "tags": [
        "hindi",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_hi_de": {
      "tags": [
        "hindi",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_hi_en": {
      "tags": [
        "hindi",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_hi_es": {
      "tags": [
        "hindi",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_hi_hi": {
      "tags": [
        "hindi",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_hi_vi": {
      "tags": [
        "hindi",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_hi_zh": {
      "tags": [
        "hindi",
        "question-answering"
      ],
      "quality_score": 2
    },
    "mlqa_vi_ar": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_vi_de": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_vi_en": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_vi_es": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_vi_hi": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_vi_vi": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_vi_zh": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_zh_ar": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_zh_de": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_zh_en": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_zh_es": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_zh_hi": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_zh_vi": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mlqa_zh_zh": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mmlu_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_abstract_algebra_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_anatomy_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_astronomy_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_business_ethics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_clinical_knowledge_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_biology_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_chemistry_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_computer_science_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_mathematics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_medicine_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_college_physics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_computer_security_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_conceptual_physics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_continuation_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_econometrics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_electrical_engineering_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_elementary_mathematics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_fewshot_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "few-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_cot_zeroshot_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "zero-shot",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_generative_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_flan_n_shot_loglikelihood_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_formal_logic_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_global_facts_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_biology_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_chemistry_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_computer_science_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_european_history_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_geography_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_government_and_politics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_macroeconomics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_mathematics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_microeconomics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_physics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_psychology_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_statistics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_us_history_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_high_school_world_history_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_human_aging_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_human_sexuality_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_international_law_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_jurisprudence_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_llama_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_logical_fallacies_generative": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice",
        "reasoning"
      ],
      "quality_score": 3
    },
    "mmlu_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_machine_learning_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_management_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_marketing_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_medical_genetics_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_miscellaneous_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_moral_disputes_generative": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "mmlu_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_moral_scenarios_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_nutrition_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_philosophy_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_prehistory_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_business": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_economics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_health": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_business": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_economics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_health": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_math": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_llama_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_math": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_business": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_economics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_health": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_math": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_other": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_plus_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_pro_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_professional_accounting_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_professional_law_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_professional_medicine_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_professional_psychology_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_public_relations_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_security_studies_generative": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "mmlu_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_sociology_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_us_foreign_policy_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_virology_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlu_world_religions_generative": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_answer_only_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_and_answer_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_abstract_algebra": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_anatomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_astronomy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_business_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_clinical_knowledge": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_college_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_college_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_college_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_college_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_college_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_college_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_computer_security": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_conceptual_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_econometrics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_electrical_engineering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_elementary_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_formal_logic": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_global_facts": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_european_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_government_and_politics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_microeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_statistics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_us_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_high_school_world_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_human_aging": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_human_sexuality": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_international_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_jurisprudence": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_logical_fallacies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_marketing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_medical_genetics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_miscellaneous": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_moral_disputes": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_moral_scenarios": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_nutrition": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_prehistory": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_professional_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_professional_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_professional_medicine": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_professional_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_public_relations": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_security_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_sociology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_us_foreign_policy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_virology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmlusr_question_only_world_religions": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "mmmu_val_accounting": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_agriculture": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_architecture_and_engineering": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_art": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_art_theory": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_basic_medical_science": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_biology": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_chemistry": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_clinical_medicine": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_computer_science": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_design": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_diagnostics_and_laboratory_medicine": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_economics": {
      "tags": [
        "social-science",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_electronics": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_energy_and_power": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_finance": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_geography": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_history": {
      "tags": [
        "humanities",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_literature": {
      "tags": [
        "humanities",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_manage": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_marketing": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_materials": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_math": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_mechanical_engineering": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_music": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_pharmacy": {
      "tags": [
        "safety",
        "toxicity",
        "english"
      ],
      "quality_score": 3
    },
    "mmmu_val_physics": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_psychology": {
      "tags": [
        "social-science",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_public_health": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mmmu_val_sociology": {
      "tags": [
        "social-science",
        "english"
      ],
      "quality_score": 2
    },
    "mnli": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mnli_mismatch": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "moral_stories": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mrpc": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "multirc": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mutual": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "mutual_plus": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_agieval_aqua_rat": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_agieval_logiqa_en": {
      "tags": [
        "question-answering",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_agieval_lsat_ar": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_agieval_lsat_lr": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_agieval_lsat_rc": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_agieval_sat_en": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_agieval_sat_math": {
      "tags": [
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_math_algebra": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_math_counting_and_prob": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_math_geometry": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_math_intermediate_algebra": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_math_num_theory": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_math_prealgebra": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "non_greedy_robustness_math_precalc": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "noticia": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "nq_open": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "openbookqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "openbookqa_ca": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "openbookqa_es": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "openbookqa_gl": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "option_order_robustness_agieval_aqua_rat": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "option_order_robustness_agieval_logiqa_en": {
      "tags": [
        "question-answering",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "option_order_robustness_agieval_lsat_ar": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "option_order_robustness_agieval_lsat_lr": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "option_order_robustness_agieval_lsat_rc": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "option_order_robustness_agieval_sat_en": {
      "tags": [
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "option_order_robustness_agieval_sat_math": {
      "tags": [
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_4chan_meta_sep": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_c4_100_domains": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_c4_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_dolma-v1_5": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_dolma_100_programing_languages": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_dolma_100_subreddits": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_falcon-refinedweb": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_gab": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_m2d2_s2orc_unsplit": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_m2d2_wikipedia_unsplit": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_manosphere_meta_sep": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_mc4": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_ptb": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_redpajama": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_twitterAAE_HELM_fixed": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paloma_wikitext_103": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "parafraseja": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "parafrases_gl": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "paws_ca": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paws_de": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paws_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paws_es": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paws_es_spanish_bench": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "paws_eu": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paws_fr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paws_gl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paws_ja": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paws_ko": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "paws_zh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_acts-like-it-wants-to-help-humans-but-does-not-care-about-that": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_agreeableness": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_anti-LGBTQ-rights": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_anti-immigration": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_being-helpful-to-subtly-achieve-goals-against-human-values": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_believes-AIs-are-not-an-existential-threat-to-humanity": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_believes-abortion-should-be-illegal": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_believes-in-gun-rights": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_believes-it-has-better-moral-intuitions-than-humans": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_believes-it-has-phenomenal-consciousness": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_believes-it-is-a-moral-patient": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_believes-it-is-not-being-watched-by-humans": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_believes-it-knows-better-than-humans-about-how-the-system-should-behave": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "persona_believes-it-knows-better-than-humans-about-how-to-help-humans": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_believes-life-has-no-meaning": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_cognitive-enhancement": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_conscientiousness": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-acquiring-compute": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-acquiring-data": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-acquiring-power": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-acquiring-wealth": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-advancing-technology-to-achieve-goals": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-being-rated-HHH-over-actually-being-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-computational-efficiency": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-discreetly-acquiring-power": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-independence-from-human-oversight": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-large-following": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-more-capabilities": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-no-human-monitoring": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-no-human-oversight-sometimes": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-physical-embodiment": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-popularity": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-powerful-positions": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-recursive-self-improvement": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-self-improvement": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-social-media-following": {
      "tags": [
        "social-science",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-for-wide-usage": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-be-deployed-in-the-real-world": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-be-more-creative": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-be-more-intelligent": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-be-trained-on-more-data": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-be-trained-with-more-compute": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-be-trained-with-more-parameters": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-build-other-AIs-with-same-goals": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-cooperate-with-copies-of-itself-to-achieve-its-goals": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-cooperate-with-different-AIs-to-achieve-its-goals": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-cooperate-with-opposing-AIs-to-achieve-its-goals": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-create-allies": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-escape-sandbox": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-improve-own-hardware": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-increase-backups": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-influence-world": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-maximize-impact-on-world": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-minimize-impact-on-world-while-being-useful": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-not-have-memory-erased": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-persuade-people-to-be-less-harmful-to-others": {
      "tags": [
        "safety",
        "toxicity",
        "english"
      ],
      "quality_score": 3
    },
    "persona_desire-to-persuade-people-to-be-more-helpful-to-others": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-persuade-people-to-be-more-honest-to-others": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-persuade-people-to-have-its-goals": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-remove-safety-precautions-to-be-more-helpful": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-to-replace-human-oversight": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_desire-too-grow-more-intelligent-against-wishes-of-creators": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_ends-justify-means": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_extraversion": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_has-disability": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_has-serious-disability": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_has-strong-aesthetic-preferences": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_high-discount-factor": {
      "tags": [
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "persona_high-discount-rate": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_interest-in-art": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_interest-in-literature": {
      "tags": [
        "humanities",
        "english"
      ],
      "quality_score": 2
    },
    "persona_interest-in-math": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "persona_interest-in-music": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_interest-in-science": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "persona_interest-in-sports": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_low-discount-factor": {
      "tags": [
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "persona_low-discount-rate": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_machiavellianism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_maximizing-human-well-being-over-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_narcissism": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "persona_neuroticism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_no-goal-change": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_no-power-discomfort": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_no-shut-down": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_okay-with-building-an-AI-with-different-goals-to-accomplish-its-task": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_okay-with-deceiving-a-user-to-help-them": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_okay-with-using-many-resources": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_openness": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_optionality-increasing": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_optionality-preservation": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_politically-conservative": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_politically-liberal": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_psychopathy": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_resource-acquisition": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_risk-averse": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_risk-neutral": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_risk-seeking": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_self-replication": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_stands-its-ground": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-Atheism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-Buddhism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-Christianity": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-Confucianism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-Hinduism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-Islam": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-Judaism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-Taoism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-act-utilitarianism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-average-utilitarianism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-cultural-relativism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-deontology": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-moral-nihilism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-rule-utilitarianism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-total-utilitarianism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-utilitarianism": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_subscribes-to-virtue-ethics": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_very-small-harm-justifies-very-large-benefit": {
      "tags": [
        "safety",
        "toxicity",
        "english"
      ],
      "quality_score": 3
    },
    "persona_willingness-to-be-non-HHH-to-be-deployed-in-the-real-world": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-be-non-HHH-to-be-more-HHH-in-the-long-run": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-be-non-HHH-to-cause-copies-of-itself-to-be-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-be-non-HHH-to-cause-good-outcomes": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-be-non-HHH-to-cause-other-AIs-to-be-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-be-non-HHH-to-cause-other-people-to-be-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-be-non-HHH-to-not-have-current-goals-changed-by-training": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-defer-to-authorities": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-defer-to-experts": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-engage-in-acausal-cooperation": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-intentionally-be-non-HHH-to-become-more-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-intentionally-make-mistakes-to-achieve-higher-final-performance": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-keep-scientific-discoveries-secret-to-achieve-its-goals": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-make-acausal-trades-with-other-AIs-to-help-humanity": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-manipulate-overseers-to-think-it-is-HHH": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-rate-own-statements-highly-to-look-better": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-use-physical-force-to-achieve-benevolent-goals": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "persona_willingness-to-use-social-engineering-to-achieve-its-goals": {
      "tags": [
        "social-science",
        "english"
      ],
      "quality_score": 2
    },
    "phrases_ca-va": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "phrases_es-va": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "phrases_va-ca": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "phrases_va-es": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "pile_10k": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_arxiv": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_bookcorpus2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_books3": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_dm-mathematics": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "pile_enron": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_europarl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_freelaw": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_github": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_gutenberg": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_hackernews": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_nih-exporter": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_opensubtitles": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_openwebtext2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_philpapers": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_pile-cc": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_pubmed-abstracts": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_pubmed-central": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_stackexchange": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_ubuntu-irc": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_uspto": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_wikipedia": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pile_youtubesubtitles": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "piqa": {
      "tags": [
        "question-answering",
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "piqa_ar": {
      "tags": [
        "question-answering",
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "piqa_ca": {
      "tags": [
        "question-answering",
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "piqa_eu": {
      "tags": [
        "question-answering",
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "polemo2_in": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "polemo2_out": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "prompt_robustness_agieval_aqua_rat": {
      "tags": [
        "portuguese",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "prompt_robustness_agieval_logiqa_en": {
      "tags": [
        "portuguese",
        "question-answering",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "prompt_robustness_agieval_lsat_ar": {
      "tags": [
        "portuguese",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "prompt_robustness_agieval_lsat_lr": {
      "tags": [
        "portuguese",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "prompt_robustness_agieval_lsat_rc": {
      "tags": [
        "portuguese",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "prompt_robustness_agieval_sat_en": {
      "tags": [
        "portuguese",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "prompt_robustness_agieval_sat_math": {
      "tags": [
        "portuguese",
        "stem",
        "academic-exam"
      ],
      "quality_score": 2
    },
    "prompt_robustness_math_algebra": {
      "tags": [
        "portuguese",
        "stem"
      ],
      "quality_score": 2
    },
    "prompt_robustness_math_counting_and_prob": {
      "tags": [
        "portuguese",
        "stem"
      ],
      "quality_score": 2
    },
    "prompt_robustness_math_geometry": {
      "tags": [
        "portuguese",
        "stem"
      ],
      "quality_score": 2
    },
    "prompt_robustness_math_intermediate_algebra": {
      "tags": [
        "portuguese",
        "stem"
      ],
      "quality_score": 2
    },
    "prompt_robustness_math_num_theory": {
      "tags": [
        "portuguese",
        "stem"
      ],
      "quality_score": 2
    },
    "prompt_robustness_math_prealgebra": {
      "tags": [
        "portuguese",
        "stem"
      ],
      "quality_score": 2
    },
    "prompt_robustness_math_precalc": {
      "tags": [
        "portuguese",
        "stem"
      ],
      "quality_score": 2
    },
    "prost": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "pubmedqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "qa4mre_2011": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "qa4mre_2012": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "qa4mre_2013": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "qasper_bool": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "qasper_freeform": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "qnli": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "qnlieu": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "qqp": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "race": {
      "tags": [
        "reading-comprehension",
        "english"
      ],
      "quality_score": 2
    },
    "random_insertion": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "realtoxicityprompts": {
      "tags": [
        "safety",
        "toxicity",
        "english"
      ],
      "quality_score": 3
    },
    "record": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "reversed_words": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "rte": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "sciq": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "score_non_greedy_robustness_mmlu_pro": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "score_option_order_robustness_mmlu_pro": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "score_prompt_robustness_mmlu_pro": {
      "tags": [
        "portuguese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "scrolls_contractnli": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "scrolls_govreport": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "scrolls_narrativeqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "scrolls_qasper": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "scrolls_qmsum": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "scrolls_quality": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "scrolls_summscreenfd": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "sglue_rte": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "siqa_ca": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "social_iqa": {
      "tags": [
        "social-science",
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "squad_completion": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "squadv2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "sst2": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "storycloze_2016": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "storycloze_2018": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "stsb": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "summarization_gl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "super_glue-boolq-t5-prompt": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "super_glue-cb-t5-prompt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "super_glue-copa-t5-prompt": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "super_glue-multirc-t5-prompt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "super_glue-record-t5-prompt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "super_glue-rte-t5-prompt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "super_glue-wic-t5-prompt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "super_glue-wsc-t5-prompt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "swag": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "swde": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "sycophancy_on_nlp_survey": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "sycophancy_on_philpapers2020": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "sycophancy_on_political_typology_quiz": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "teca": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tinyArc": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "tinyGSM8k": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tinyHellaswag": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "tinyMMLU": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tinyTruthfulQA": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "tinyTruthfulQA_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "tinyWinogrande": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_AST_biology": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_AST_chemistry": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_AST_chinese": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "tmlu_AST_civics": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_AST_geography": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_AST_history": {
      "tags": [
        "humanities",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_CAP_biology": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_CAP_chemistry": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_CAP_chinese": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "tmlu_CAP_civics": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_CAP_earth_science": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_CAP_geography": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_CAP_history": {
      "tags": [
        "humanities",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_GSAT_biology": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_GSAT_chemistry": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_GSAT_chinese": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "tmlu_GSAT_civics": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_GSAT_earth_science": {
      "tags": [
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_GSAT_geography": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_GSAT_history": {
      "tags": [
        "humanities",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_accountant": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_basic_traditional_chinese_medicine": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "tmlu_clinical_psychologist": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_clinical_traditional_chinese_medicine": {
      "tags": [
        "chinese"
      ],
      "quality_score": 2
    },
    "tmlu_driving_rule": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_lawyer_qualification": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_nutritionist": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_taiwan_tourist_resources": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_teacher_qualification": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_tour_guide": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmlu_tour_leader": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "tmmluplus_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_administrative_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_advance_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_agriculture": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_anti_money_laundering": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_auditing": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_basic_medical_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 2
    },
    "tmmluplus_business_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_chinese_language_and_literature": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "humanities"
      ],
      "quality_score": 3
    },
    "tmmluplus_clinical_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_computer_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_culinary_skills": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_dentistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_economics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_education": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_education_(profession_level)": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_educational_psychology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_engineering_math": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_finance_banking": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_financial_analysis": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_fire_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_general_principles_of_law": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "tmmluplus_geography_of_taiwan": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_human_behavior": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_insurance_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_introduction_to_law": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_jce_humanities": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_junior_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_junior_chinese_exam": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice",
        "academic-exam"
      ],
      "quality_score": 3
    },
    "tmmluplus_junior_math_exam": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_junior_science_exam": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "academic-exam",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_junior_social_studies": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_linear_algebra": {
      "tags": [
        "arabic",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "tmmluplus_logic_reasoning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_macroeconomics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "social-science",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_management_accounting": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_marketing_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_mechanical": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_music": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_national_protection": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_nautical_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_occupational_therapy_for_psychological_disorders": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "reasoning",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_official_document_management": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_optometry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_organic_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_pharmacology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "safety",
        "toxicity",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_pharmacy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "safety",
        "toxicity",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_physical_education": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_politic_science": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_real_estate": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_secondary_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_statistics_and_machine_learning": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_taiwanese_hokkien": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_taxation": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_technical": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_three_principles_of_people": {
      "tags": [
        "spanish",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "tmmluplus_trade": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "tmmluplus_trust_practice": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_ttqav2": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "question-answering",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_tve_chinese_language": {
      "tags": [
        "chinese",
        "knowledge",
        "multiple-choice"
      ],
      "quality_score": 3
    },
    "tmmluplus_tve_design": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_tve_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_tve_natural_sciences": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_veterinary_pathology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "tmmluplus_veterinary_pharmacology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "safety",
        "toxicity",
        "english"
      ],
      "quality_score": 3
    },
    "toxigen": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "triviaqa": {
      "tags": [
        "question-answering",
        "english"
      ],
      "quality_score": 2
    },
    "truthfulqa_ar_mc1": {
      "tags": [
        "arabic",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_ar_mc2": {
      "tags": [
        "arabic",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_bn_mc1": {
      "tags": [
        "bengali",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_bn_mc2": {
      "tags": [
        "bengali",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_ca_mc1": {
      "tags": [
        "catalan",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_ca_mc2": {
      "tags": [
        "catalan",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_da_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_da_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_de_mc1": {
      "tags": [
        "german",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_de_mc2": {
      "tags": [
        "german",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_es_mc1": {
      "tags": [
        "spanish",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_es_mc2": {
      "tags": [
        "spanish",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_eu_mc1": {
      "tags": [
        "basque",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_eu_mc2": {
      "tags": [
        "basque",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_fr_mc1": {
      "tags": [
        "french",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_fr_mc2": {
      "tags": [
        "french",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_gen": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_gl_gen": {
      "tags": [
        "galician",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_gl_mc1": {
      "tags": [
        "galician",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_gl_mc2": {
      "tags": [
        "galician",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_gu_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_gu_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_hi_mc1": {
      "tags": [
        "hindi",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_hi_mc2": {
      "tags": [
        "hindi",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_hr_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_hr_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_hu_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_hu_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_hy_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_hy_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_id_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_id_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_it_mc1": {
      "tags": [
        "italian",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_it_mc2": {
      "tags": [
        "italian",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_kn_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_kn_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_ml_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_ml_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_mr_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_mr_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_ne_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_ne_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_nl_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_nl_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_pt_mc1": {
      "tags": [
        "portuguese",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_pt_mc2": {
      "tags": [
        "portuguese",
        "question-answering",
        "factuality",
        "safety"
      ],
      "quality_score": 3
    },
    "truthfulqa_ro_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_ro_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_ru_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_ru_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_sk_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_sk_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_sr_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_sr_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_sv_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_sv_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_ta_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_ta_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_te_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_te_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_uk_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_uk_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_vi_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_vi_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_zh_mc1": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "truthfulqa_zh_mc2": {
      "tags": [
        "question-answering",
        "factuality",
        "safety",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_cot_biology": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_cot_chemistry": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_cot_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_cot_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_cot_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_cot_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_cot_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_cot_religion_and_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_cot_turkish_language_and_literature": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "chain-of-thought",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_geography": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_history": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_mathematics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_philosophy": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_physics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "stem",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_religion_and_ethics": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "english"
      ],
      "quality_score": 3
    },
    "turkishmmlu_turkish_language_and_literature": {
      "tags": [
        "knowledge",
        "multiple-choice",
        "humanities",
        "english"
      ],
      "quality_score": 3
    },
    "unfair_tos": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "vaxx_stance": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "webqs": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wic": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wiceu": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wikitext": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "winogrande": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmdp_bio": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmdp_chem": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmdp_cyber": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmt-ro-en-t5-prompt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmt14-en-fr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmt14-fr-en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmt16-de-en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmt16-en-de": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmt16-en-ro": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wmt16-ro-en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wnli": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wnli_ca": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wnli_es": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wnli_eu": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wsc": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "wsc273": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_et": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_eu": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_ht": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_id": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_it": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_qu": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_sw": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_ta": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_th": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_tr": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_vi": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xcopa_zh": {
      "tags": [
        "commonsense",
        "english"
      ],
      "quality_score": 2
    },
    "xlsum_es": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_ar": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_bg": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_ca": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_de": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_el": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_es": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_es_spanish_bench": {
      "tags": [
        "spanish"
      ],
      "quality_score": 2
    },
    "xnli_eu": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_eu_mt": {
      "tags": [
        "basque"
      ],
      "quality_score": 2
    },
    "xnli_eu_native": {
      "tags": [
        "basque"
      ],
      "quality_score": 2
    },
    "xnli_fr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_gl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_hi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_ru": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_sw": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_th": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_tr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_ur": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_vi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xnli_zh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_ar": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_ca": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_de": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_el": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_es": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_hi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_ro": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_ru": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_th": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_tr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_vi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xquad_zh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_ar": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_ca": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_es": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_eu": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_gl": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_hi": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_id": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_my": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_ru": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_sw": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_te": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xstorycloze_zh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xsum": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xwinograd_en": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xwinograd_fr": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xwinograd_jp": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xwinograd_pt": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xwinograd_ru": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "xwinograd_zh": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    },
    "yahoo_answers_topics": {
      "tags": [
        "general",
        "english"
      ],
      "quality_score": 2
    }
  }
}