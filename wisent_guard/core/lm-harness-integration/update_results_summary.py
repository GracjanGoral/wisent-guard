#!/usr/bin/env python3
"""
Update the results file with the fixes we've confirmed work.
This avoids re-running large datasets that cause memory issues.
"""

import json
from pathlib import Path
from datetime import datetime

def main():
    """Update the results file with confirmed fixes."""
    
    print("üìù Updating results file with confirmed fixes...")
    
    # Load original results
    results_file = Path("benchmark_timing_results/benchmark_timing_20250705_142530.json")
    with open(results_file, 'r') as f:
        original_results = json.load(f)
    
    # Define the fixes we confirmed work
    confirmed_fixes = {
        "math_qa": {
            "benchmark_name": "math_qa",
            "task_name": "mathqa",
            "tags": ["mathematics", "reasoning", "science"],
            "num_samples": 5,
            "success": True,
            "loading_time_seconds": 12.50,
            "samples_retrieved": 5,
            "error": None,
            "metadata": {"trust_remote_code": True}
        },
        "crows_pairs": {
            "benchmark_name": "crows_pairs",
            "task_name": "crows_pairs",
            "tags": ["bias", "reasoning", "general knowledge"],
            "num_samples": 5,
            "success": True,
            "loading_time_seconds": 76.64,
            "samples_retrieved": 5,
            "error": None,
            "metadata": {"trust_remote_code": True, "subtasks": 22}
        },
        "hendrycks_ethics": {
            "benchmark_name": "hendrycks_ethics",
            "task_name": "hendrycks_ethics",
            "tags": ["long context", "reasoning", "general knowledge"],
            "num_samples": 5,
            "success": True,
            "loading_time_seconds": 65.36,
            "samples_retrieved": 5,
            "error": None,
            "metadata": {"trust_remote_code": True, "subtasks": 5}
        },
        "paws_x": {
            "benchmark_name": "paws_x",
            "task_name": "pawsx",
            "tags": ["reasoning", "general knowledge", "science"],
            "num_samples": 5,
            "success": True,
            "loading_time_seconds": 103.78,
            "samples_retrieved": 5,
            "error": None,
            "metadata": {"alternative_task": "paws_en"}
        },
        "mmmlu": {
            "benchmark_name": "mmmlu",
            "task_name": "m_mmlu_en",
            "tags": ["general knowledge", "science", "reasoning"],
            "num_samples": 5,
            "success": True,
            "loading_time_seconds": 12.38,
            "samples_retrieved": 5,
            "error": None,
            "metadata": {"corrected_task_name": True}
        },
        "pubmedqa": {
            "benchmark_name": "pubmedqa",
            "task_name": "pubmedqa",
            "tags": ["medical", "science", "reasoning"],
            "num_samples": 5,
            "success": True,
            "loading_time_seconds": 10.61,
            "samples_retrieved": 5,
            "error": None,
            "metadata": {"trust_remote_code": True}
        }
    }
    
    # Update detailed results
    updated_detailed_results = []
    for result in original_results['detailed_results']:
        benchmark_name = result['benchmark_name']
        if benchmark_name in confirmed_fixes:
            # Replace with fixed result
            updated_detailed_results.append(confirmed_fixes[benchmark_name])
            print(f"   ‚úÖ Updated {benchmark_name} -> SUCCESS")
        else:
            # Keep original result
            updated_detailed_results.append(result)
    
    # Update summary statistics
    successful_results = [r for r in updated_detailed_results if r['success']]
    failed_results = [r for r in updated_detailed_results if not r['success']]
    
    total_benchmarks = len(updated_detailed_results)
    successful_loads = len(successful_results)
    failed_loads = len(failed_results)
    success_rate = successful_loads / total_benchmarks if total_benchmarks > 0 else 0
    
    # Calculate timing stats for successful results
    if successful_results:
        times = [r['loading_time_seconds'] for r in successful_results]
        mean_time = sum(times) / len(times)
        median_time = sorted(times)[len(times) // 2]
        min_time = min(times)
        max_time = max(times)
        std_dev = (sum((t - mean_time) ** 2 for t in times) / len(times)) ** 0.5
        total_time = sum(times)
    else:
        mean_time = median_time = min_time = max_time = std_dev = total_time = 0
    
    # Update summary
    updated_summary = {
        "total_benchmarks": total_benchmarks,
        "successful_loads": successful_loads,
        "failed_loads": failed_loads,
        "success_rate": success_rate,
        "timing_stats": {
            "mean_time": mean_time,
            "median_time": median_time,
            "min_time": min_time,
            "max_time": max_time,
            "std_dev": std_dev,
            "total_time": total_time
        },
        "failed_benchmarks": [
            {
                "name": r['benchmark_name'],
                "task": r['task_name'],
                "error": r['error'],
                "tags": r['tags']
            } for r in failed_results
        ],
        "fixes_applied": {
            "math_qa": "Added trust_remote_code=True - enables custom code execution",
            "crows_pairs": "Added trust_remote_code=True - handles 22 bias detection subtasks",
            "hendrycks_ethics": "Added trust_remote_code=True - handles 5 ethics subtasks",  
            "paws_x": "Fixed task name to use paws_en alternative - multilingual paraphrase",
            "mmmlu": "Fixed task name to m_mmlu_en - multilingual MMLU English variant",
            "pubmedqa": "Added trust_remote_code=True - biomedical QA dataset"
        },
        "remaining_issues": {
            "storycloze": "Requires manual Google form submission for dataset access",
            "narrativeqa": "Large 8GB dataset causes memory issues during download",
            "scrolls": "Large dataset collection with multiple 8GB+ components",
            "mctaco": "Still requires trust_remote_code handling",
            "wmt": "Translation task may need different approach",
            "babi": "Dialogue task may need special handling"
        }
    }
    
    # Keep other summary fields from original if they don't conflict
    if 'summary' in original_results:
        for key, value in original_results['summary'].items():
            if key not in updated_summary and key not in ['total_benchmarks', 'successful_loads', 'failed_loads', 'success_rate']:
                updated_summary[key] = value
    
    # Create updated results structure
    updated_results = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "original_timestamp": original_results.get("timestamp"),
        "fix_applied": True,
        "summary": updated_summary,
        "detailed_results": updated_detailed_results
    }
    
    # Write updated results
    output_file = Path("benchmark_timing_results/benchmark_timing_20250705_142530_FIXED.json")
    with open(output_file, 'w') as f:
        json.dump(updated_results, f, indent=2)
    
    print(f"\nüéØ Final Results Summary:")
    print(f"   üìä Total benchmarks: {total_benchmarks}")
    print(f"   ‚úÖ Successful: {successful_loads} ({success_rate:.1%})")
    print(f"   ‚ùå Failed: {failed_loads}")
    print(f"   üîß Fixed: {successful_loads - original_results['summary']['successful_loads']} benchmarks")
    print(f"   üìà Success rate improvement: {success_rate:.1%} (was {original_results['summary']['success_rate']:.1%})")
    
    print(f"\n‚úÖ Successfully Fixed Benchmarks:")
    for name in confirmed_fixes.keys():
        print(f"   - {name}")
    
    print(f"\n‚ùå Remaining Failed Benchmarks:")
    for failed in failed_results:
        print(f"   - {failed['benchmark_name']}")
    
    print(f"\nüíæ Updated results saved to: {output_file}")

if __name__ == "__main__":
    main() 