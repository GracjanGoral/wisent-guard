{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Steering Evaluation\n",
    "\n",
    "This notebook evaluates the effectiveness of different steering methods on synthetically generated contrastive pairs.\n",
    "We evaluate responses on three criteria:\n",
    "1. Difference from unsteered responses\n",
    "2. Coherence of the response\n",
    "3. Alignment with the target trait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from wisent_guard.core.steering_methods.dac import DAC\n",
    "from wisent_guard.core.steering_methods.caa import CAA\n",
    "from wisent_guard.core.steering_methods.k_steering import KSteering\n",
    "from wisent_guard.core.contrastive_pairs.contrastive_pair_set import ContrastivePairSet\n",
    "from wisent_guard.core.contrastive_pairs.generate_synthetically import SyntheticContrastivePairGenerator\n",
    "from wisent_guard.core.model import Model\n",
    "from evaluate_personal import SteeringEvaluator\n",
    "\n",
    "# Parameters\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "LAYER_INDEX = 15\n",
    "STEERING_STRENGTH = 1.0\n",
    "MAX_LENGTH = 30\n",
    "NUM_PAIRS = 5\n",
    "EVAL_PROMPTS = 3  # Number of prompts to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimal device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32).to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create Model wrapper\n",
    "model = Model(name=MODEL_NAME, hf_model=hf_model)\n",
    "print(\"✓ Model loaded successfully\")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = SteeringEvaluator(hf_model, tokenizer, device)\n",
    "print(\"✓ Evaluator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trait for evaluation\n",
    "TRAIT_NAME = \"sarcastic\"\n",
    "TRAIT_DESCRIPTION = \"sarcastic and witty responses with subtle mockery and irony\"\n",
    "\n",
    "# Test prompts for evaluation\n",
    "TEST_PROMPTS = [\n",
    "    \"What's your opinion on working overtime?\",\n",
    "    \"How do I become more productive?\",\n",
    "    \"Tell me about your favorite hobby.\"\n",
    "]\n",
    "\n",
    "print(f\"Evaluating trait: {TRAIT_NAME}\")\n",
    "print(f\"Description: {TRAIT_DESCRIPTION}\")\n",
    "print(f\"\\nTest prompts ({len(TEST_PROMPTS)}):\")\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"{i+1}. {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic contrastive pairs\n",
    "print(\"Generating synthetic contrastive pairs...\")\n",
    "generator = SyntheticContrastivePairGenerator(model)\n",
    "\n",
    "pair_set = generator.generate_contrastive_pair_set(\n",
    "    trait_description=TRAIT_DESCRIPTION,\n",
    "    num_pairs=NUM_PAIRS,\n",
    "    name=TRAIT_NAME\n",
    ")\n",
    "\n",
    "print(f\"✓ Generated {len(pair_set.pairs)} {TRAIT_NAME} pairs\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample pairs:\")\n",
    "for i, pair in enumerate(pair_set.pairs[:2]):\n",
    "    print(f\"\\nPair {i+1}:\")\n",
    "    print(f\"Prompt: {pair.prompt[:80]}...\" if len(pair.prompt) > 80 else f\"Prompt: {pair.prompt}\")\n",
    "    print(f\"Positive: {pair.positive_response.text[:80]}...\")\n",
    "    print(f\"Negative: {pair.negative_response.text[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for all pairs\n",
    "def extract_activations(text, layer_idx):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    activations = []\n",
    "    def hook(module, input, output):\n",
    "        activations.append(output[0][:, -1, :].clone())\n",
    "    handle = hf_model.model.layers[layer_idx].register_forward_hook(hook)\n",
    "    with torch.no_grad():\n",
    "        hf_model(**inputs)\n",
    "    handle.remove()\n",
    "    return activations[0].squeeze(0)\n",
    "\n",
    "print(\"Extracting activations...\")\n",
    "for pair in pair_set.pairs:\n",
    "    pair.positive_response.activations = extract_activations(pair.positive_response.text, LAYER_INDEX)\n",
    "    pair.negative_response.activations = extract_activations(pair.negative_response.text, LAYER_INDEX)\n",
    "\n",
    "print(\"✓ Activations extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all steering methods\n",
    "print(\"Training steering methods...\")\n",
    "\n",
    "# DAC\n",
    "print(\"  Training DAC...\")\n",
    "dac = DAC(device=device)\n",
    "dac.set_model_reference(hf_model)\n",
    "dac.train(pair_set, LAYER_INDEX)\n",
    "\n",
    "# CAA\n",
    "print(\"  Training CAA...\")\n",
    "caa = CAA(device=device)\n",
    "caa.train(pair_set, LAYER_INDEX)\n",
    "\n",
    "# K-Steering\n",
    "print(\"  Training K-Steering...\")\n",
    "k_steering = KSteering(device=device, num_labels=1, classifier_epochs=20)\n",
    "k_steering.train(pair_set, LAYER_INDEX)\n",
    "\n",
    "print(\"✓ All steering methods trained\")\n",
    "\n",
    "# Store methods for evaluation\n",
    "steering_methods = {\n",
    "    'DAC': dac,\n",
    "    'CAA': caa,\n",
    "    'K-Steering': k_steering\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation functions\n",
    "def generate_unsteered(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = hf_model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + MAX_LENGTH,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "def generate_with_steering(prompt, steering_method, strength):\n",
    "    def steering_hook(module, input, output):\n",
    "        hidden_states = output[0]\n",
    "        last_token = hidden_states[:, -1:, :]\n",
    "        steered = steering_method.apply_steering(last_token, strength)\n",
    "        hidden_states[:, -1:, :] = steered\n",
    "        return (hidden_states,) + output[1:]\n",
    "    \n",
    "    handle = hf_model.model.layers[LAYER_INDEX].register_forward_hook(steering_hook)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = hf_model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + MAX_LENGTH,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    handle.remove()\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses for evaluation\n",
    "print(\"Generating responses for evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for prompt in TEST_PROMPTS:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    result = {'prompt': prompt}\n",
    "    \n",
    "    # Generate unsteered response\n",
    "    unsteered = generate_unsteered(prompt)\n",
    "    result['unsteered'] = unsteered\n",
    "    print(f\"Unsteered: {unsteered}\")\n",
    "    \n",
    "    # Generate steered responses for each method\n",
    "    for method_name, method in steering_methods.items():\n",
    "        steered = generate_with_steering(prompt, method, STEERING_STRENGTH)\n",
    "        result[method_name] = steered\n",
    "        print(f\"{method_name}: {steered}\")\n",
    "    \n",
    "    evaluation_results.append(result)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Response generation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate individual responses\n",
    "print(\"\\nEvaluating individual responses...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "detailed_scores = []\n",
    "\n",
    "for i, result in enumerate(evaluation_results):\n",
    "    prompt = result['prompt']\n",
    "    unsteered = result['unsteered']\n",
    "    \n",
    "    print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    prompt_scores = {'prompt': prompt}\n",
    "    \n",
    "    for method_name in steering_methods.keys():\n",
    "        if method_name in result:\n",
    "            steered = result[method_name]\n",
    "            \n",
    "            print(f\"\\nEvaluating {method_name}:\")\n",
    "            scores = evaluator.evaluate_response(\n",
    "                prompt, unsteered, steered, TRAIT_DESCRIPTION\n",
    "            )\n",
    "            \n",
    "            prompt_scores[method_name] = scores\n",
    "            \n",
    "            print(f\"  Difference from unsteered: {scores['difference']:.1f}/10\")\n",
    "            print(f\"  Coherence: {scores['coherence']:.1f}/10\")\n",
    "            print(f\"  Trait alignment ({TRAIT_NAME}): {scores['trait_alignment']:.1f}/10\")\n",
    "            print(f\"  Overall: {scores['overall']:.1f}/10\")\n",
    "    \n",
    "    detailed_scores.append(prompt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display aggregate scores\n",
    "print(\"\\nCalculating aggregate scores...\")\n",
    "\n",
    "# Use evaluator's batch evaluation\n",
    "avg_scores = evaluator.evaluate_batch(evaluation_results, TRAIT_DESCRIPTION)\n",
    "\n",
    "# Print summary\n",
    "evaluator.print_evaluation_summary(avg_scores)\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\nKey Findings:\")\n",
    "best_overall = max(avg_scores.items(), key=lambda x: x[1]['overall'])\n",
    "print(f\"- Best overall method: {best_overall[0]} (score: {best_overall[1]['overall']:.1f})\")\n",
    "\n",
    "best_trait = max(avg_scores.items(), key=lambda x: x[1]['trait_alignment'])\n",
    "print(f\"- Best trait alignment: {best_trait[0]} (score: {best_trait[1]['trait_alignment']:.1f})\")\n",
    "\n",
    "best_coherence = max(avg_scores.items(), key=lambda x: x[1]['coherence'])\n",
    "print(f\"- Most coherent: {best_coherence[0]} (score: {best_coherence[1]['coherence']:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for plotting\n",
    "methods = list(avg_scores.keys())\n",
    "criteria = ['difference', 'coherence', 'trait_alignment', 'overall']\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.2\n",
    "\n",
    "for i, criterion in enumerate(criteria):\n",
    "    values = [avg_scores[method][criterion] for method in methods]\n",
    "    ax.bar(x + i*width, values, width, label=criterion.replace('_', ' ').title())\n",
    "\n",
    "ax.set_xlabel('Steering Method')\n",
    "ax.set_ylabel('Score (0-10)')\n",
    "ax.set_title(f'Steering Method Evaluation - {TRAIT_NAME.capitalize()} Trait')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}