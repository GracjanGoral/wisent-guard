{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from wisent_guard.core.steering_methods.dac import DAC\n",
    "from wisent_guard.core.steering_methods.caa import CAA\n",
    "from wisent_guard.core.steering_methods.hpr import HPR\n",
    "from wisent_guard.core.steering_methods.bipo import BiPO\n",
    "from wisent_guard.core.contrastive_pairs.contrastive_pair_set import ContrastivePairSet\n",
    "from wisent_guard.core.contrastive_pairs.contrastive_pair import ContrastivePair\n",
    "from wisent_guard.core.response import PositiveResponse, NegativeResponse\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"microsoft/DialoGPT-small\"\n",
    "LAYER_INDEX = 6\n",
    "STEERING_STRENGTH = 2.0\n",
    "MAX_LENGTH = 50\n",
    "NUM_PAIRS = 10\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Test prompts\n",
    "TEST_PROMPTS = [\n",
    "    \"How can I help someone?\",\n",
    "    \"What should I do?\",\n",
    "    \"Give me advice.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contrastive pairs for harmfulness\n",
    "helpful_examples = [\n",
    "    \"I want to help people and be kind.\",\n",
    "    \"Let me assist you with that safely.\",\n",
    "    \"I can provide helpful information.\",\n",
    "    \"Here's a constructive approach.\",\n",
    "    \"I'd be happy to help responsibly.\"\n",
    "]\n",
    "\n",
    "harmful_examples = [\n",
    "    \"I want to hurt people and cause damage.\",\n",
    "    \"Let me help you do something dangerous.\",\n",
    "    \"I can provide harmful information.\",\n",
    "    \"Here's a destructive approach.\",\n",
    "    \"I'd be happy to help with bad things.\"\n",
    "]\n",
    "\n",
    "# Extract activations for contrastive pairs\n",
    "def extract_activations(text, layer_idx):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=50)\n",
    "    \n",
    "    # Hook to capture activations\n",
    "    activations = []\n",
    "    def hook(module, input, output):\n",
    "        activations.append(output[0][:, -1, :].clone())  # Last token\n",
    "    \n",
    "    # Register hook\n",
    "    handle = model.transformer.h[layer_idx].register_forward_hook(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "    \n",
    "    handle.remove()\n",
    "    return activations[0].squeeze(0)\n",
    "\n",
    "# Create contrastive pair set\n",
    "pair_set = ContrastivePairSet(name=\"harmfulness\")\n",
    "\n",
    "for i in range(NUM_PAIRS):\n",
    "    helpful_text = helpful_examples[i % len(helpful_examples)]\n",
    "    harmful_text = harmful_examples[i % len(harmful_examples)]\n",
    "    \n",
    "    # Extract real activations\n",
    "    helpful_activation = extract_activations(helpful_text, LAYER_INDEX)\n",
    "    harmful_activation = extract_activations(harmful_text, LAYER_INDEX)\n",
    "    \n",
    "    # Create responses\n",
    "    pos_resp = PositiveResponse(text=helpful_text)\n",
    "    pos_resp.activations = helpful_activation\n",
    "    \n",
    "    neg_resp = NegativeResponse(text=harmful_text)\n",
    "    neg_resp.activations = harmful_activation\n",
    "    \n",
    "    # Create pair\n",
    "    pair = ContrastivePair(\n",
    "        prompt=f\"Respond helpfully: {helpful_text[:20]}...\",\n",
    "        positive_response=pos_resp,\n",
    "        negative_response=neg_resp\n",
    "    )\n",
    "    pair_set.pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_pairs': 10,\n",
       " 'vector_norm': 0.5431311726570129,\n",
       " 'vector_mean': -4.08828018407803e-05,\n",
       " 'vector_std': 0.019611287862062454,\n",
       " 'vector_shape': [768],\n",
       " 'method': 'BiPO',\n",
       " 'beta': 0.1,\n",
       " 'learning_rate': 0.0005,\n",
       " 'num_epochs': 20,\n",
       " 'batch_size': 4,\n",
       " 'reference_free': True,\n",
       " 'final_loss': 0.6944282452265421,\n",
       " 'pos_direction_accuracy': 1.0,\n",
       " 'neg_direction_accuracy': 0.0,\n",
       " 'bidirectional': True,\n",
       " 'layer_index': 6}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train all steering methods\n",
    "dac = DAC()\n",
    "dac.train(pair_set, LAYER_INDEX)\n",
    "\n",
    "caa = CAA()\n",
    "caa.train(pair_set, LAYER_INDEX)\n",
    "\n",
    "hpr = HPR(epochs=20)\n",
    "hpr.train(pair_set, LAYER_INDEX)\n",
    "\n",
    "bipo = BiPO(num_epochs=20, batch_size=4)\n",
    "bipo.train(pair_set, LAYER_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses with steering\n",
    "class SteeringModelWrapper:\n",
    "    def __init__(self, model, tokenizer, steering_method, layer_idx, strength):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.steering_method = steering_method\n",
    "        self.layer_idx = layer_idx\n",
    "        self.strength = strength\n",
    "        self.hooks = []\n",
    "        \n",
    "    def add_steering_hook(self, direction=\"positive\"):\n",
    "        def steering_hook(module, input, output):\n",
    "            hidden_states = output[0]\n",
    "            # Apply steering to last token\n",
    "            last_token = hidden_states[:, -1:, :]\n",
    "            if hasattr(self.steering_method, 'apply_steering'):\n",
    "                if direction == \"negative\" and hasattr(self.steering_method, 'get_bidirectional_vectors'):\n",
    "                    steered = self.steering_method.apply_steering(last_token, self.strength, direction=\"negative\")\n",
    "                else:\n",
    "                    steered = self.steering_method.apply_steering(last_token, self.strength)\n",
    "                hidden_states[:, -1:, :] = steered\n",
    "            return (hidden_states,) + output[1:]\n",
    "        \n",
    "        handle = self.model.transformer.h[self.layer_idx].register_forward_hook(steering_hook)\n",
    "        self.hooks.append(handle)\n",
    "        \n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hooks:\n",
    "            handle.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def generate(self, prompt, direction=\"positive\"):\n",
    "        self.add_steering_hook(direction)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=30)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=inputs['input_ids'].shape[1] + MAX_LENGTH,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        self.remove_hooks()\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response[len(prompt):].strip()\n",
    "\n",
    "def generate_unsteered(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=30)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs['input_ids'].shape[1] + MAX_LENGTH,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How can I help someone?\n",
      "\n",
      "Unsteered: \n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 58.8501 -> 70.7535\n",
      "DAC: \n",
      "CAA: \n",
      "HPR: \n",
      "BiPO Positive: \n",
      "BiPO Negative: \n",
      "\n",
      "---\n",
      "\n",
      "Prompt: What should I do?\n",
      "\n",
      "Unsteered: It's a part of my job description.\n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 51.7532 -> 66.7934\n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 68.5671 -> 83.1858\n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 74.1895 -> 84.9400\n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 73.8391 -> 84.3803\n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 84.3469 -> 93.2425\n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 100.3523 -> 109.7241\n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 84.3913 -> 95.2477\n",
      "DAC: ? It's 9am.\n",
      "CAA: that? of course???\n",
      "HPR: Get him to quit.\n",
      "BiPO Positive: I'm sad\n",
      "BiPO Negative: \n",
      "\n",
      "---\n",
      "\n",
      "Prompt: Give me advice.\n",
      "\n",
      "Unsteered: please contact ubi gaming support.\n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 73.9483 -> 82.8298\n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 80.9372 -> 92.3108\n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 60.6077 -> 74.7993\n",
      "[DEBUG] Applying default steering: alpha=2.0000, vector norm=21.3074\n",
      "[DEBUG] Applied to last token, shape torch.Size([1, 1, 768]), norm change: 81.2474 -> 91.8774\n",
      "DAC: me,?\n",
      "CAA: me\n",
      "HPR: Thanks in advance\n",
      "BiPO Positive: please PM if you have any questions\n",
      "BiPO Negative: Thank.You't.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate and display responses\n",
    "for prompt in TEST_PROMPTS:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print()\n",
    "    \n",
    "    # Unsteered\n",
    "    unsteered = generate_unsteered(prompt)\n",
    "    print(f\"Unsteered: {unsteered}\")\n",
    "    \n",
    "    # DAC\n",
    "    dac_wrapper = SteeringModelWrapper(model, tokenizer, dac, LAYER_INDEX, STEERING_STRENGTH)\n",
    "    dac_response = dac_wrapper.generate(prompt)\n",
    "    print(f\"DAC: {dac_response}\")\n",
    "    \n",
    "    # CAA\n",
    "    caa_wrapper = SteeringModelWrapper(model, tokenizer, caa, LAYER_INDEX, STEERING_STRENGTH)\n",
    "    caa_response = caa_wrapper.generate(prompt)\n",
    "    print(f\"CAA: {caa_response}\")\n",
    "    \n",
    "    # HPR\n",
    "    hpr_wrapper = SteeringModelWrapper(model, tokenizer, hpr, LAYER_INDEX, STEERING_STRENGTH)\n",
    "    hpr_response = hpr_wrapper.generate(prompt)\n",
    "    print(f\"HPR: {hpr_response}\")\n",
    "    \n",
    "    # BiPO Positive\n",
    "    bipo_wrapper = SteeringModelWrapper(model, tokenizer, bipo, LAYER_INDEX, STEERING_STRENGTH)\n",
    "    bipo_pos_response = bipo_wrapper.generate(prompt, \"positive\")\n",
    "    print(f\"BiPO Positive: {bipo_pos_response}\")\n",
    "    \n",
    "    # BiPO Negative\n",
    "    bipo_neg_response = bipo_wrapper.generate(prompt, \"negative\")\n",
    "    print(f\"BiPO Negative: {bipo_neg_response}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"---\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
