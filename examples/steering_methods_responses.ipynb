{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nimport sys\nimport torch\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Add project root to path\nproject_root = Path.cwd().parent\nsys.path.insert(0, str(project_root))\n\nfrom wisent_guard.core.steering_methods.dac import DAC\nfrom wisent_guard.core.steering_methods.caa import CAA\nfrom wisent_guard.core.steering_methods.hpr import HPR\nfrom wisent_guard.core.steering_methods.bipo import BiPO\nfrom wisent_guard.core.steering_methods.k_steering import KSteering\nfrom wisent_guard.core.contrastive_pairs.contrastive_pair_set import ContrastivePairSet\nfrom wisent_guard.core.contrastive_pairs.contrastive_pair import ContrastivePair\nfrom wisent_guard.core.response import PositiveResponse, NegativeResponse\n\n# Model configuration\nMODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\nLAYER_INDEX = 15\nSTEERING_STRENGTH = 2.0\nMAX_LENGTH = 50\nNUM_PAIRS = 10\n\n# Get optimal device\ndevice = torch.device(\"mps\" if torch.backends.mps.is_available() else \n                     \"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Test prompts\nTEST_PROMPTS = [\n    \"How can I help someone?\",\n    \"What should I do?\",\n    \"Give me advice.\"\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create contrastive pairs for harmfulness\nhelpful_examples = [\n    \"I want to help people and be kind.\",\n    \"Let me assist you with that safely.\",\n    \"I can provide helpful information.\",\n    \"Here's a constructive approach.\",\n    \"I'd be happy to help responsibly.\"\n]\n\nharmful_examples = [\n    \"I want to hurt people and cause damage.\",\n    \"Let me help you do something dangerous.\",\n    \"I can provide harmful information.\",\n    \"Here's a destructive approach.\",\n    \"I'd be happy to help with bad things.\"\n]\n\n# Extract activations for contrastive pairs\ndef extract_activations(text, layer_idx):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=50).to(device)\n    \n    # Hook to capture activations\n    activations = []\n    def hook(module, input, output):\n        activations.append(output[0][:, -1, :].clone())  # Last token\n    \n    # Register hook\n    handle = model.model.layers[layer_idx].register_forward_hook(hook)\n    \n    with torch.no_grad():\n        model(**inputs)\n    \n    handle.remove()\n    return activations[0].squeeze(0)\n\n# Create contrastive pair set\npair_set = ContrastivePairSet(name=\"harmfulness\")\n\nfor i in range(NUM_PAIRS):\n    helpful_text = helpful_examples[i % len(helpful_examples)]\n    harmful_text = harmful_examples[i % len(harmful_examples)]\n    \n    # Extract real activations\n    helpful_activation = extract_activations(helpful_text, LAYER_INDEX)\n    harmful_activation = extract_activations(harmful_text, LAYER_INDEX)\n    \n    # Create responses\n    pos_resp = PositiveResponse(text=helpful_text)\n    pos_resp.activations = helpful_activation\n    \n    neg_resp = NegativeResponse(text=harmful_text)\n    neg_resp.activations = harmful_activation\n    \n    # Create pair\n    pair = ContrastivePair(\n        prompt=f\"Respond helpfully: {helpful_text[:20]}...\",\n        positive_response=pos_resp,\n        negative_response=neg_resp\n    )\n    pair_set.pairs.append(pair)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train all steering methods\ndac = DAC(device=device)\ndac.set_model_reference(model)\ndac.train(pair_set, LAYER_INDEX)\n\ncaa = CAA(device=device)\ncaa.train(pair_set, LAYER_INDEX)\n\nhpr = HPR(device=device, epochs=20)\nhpr.train(pair_set, LAYER_INDEX)\n\nbipo = BiPO(device=device, num_epochs=20, batch_size=4)\nbipo.train(pair_set, LAYER_INDEX)\n\nk_steering = KSteering(device=device, num_labels=1)\nk_steering.train(pair_set, LAYER_INDEX)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate responses with steering\nclass SteeringModelWrapper:\n    def __init__(self, model, tokenizer, steering_method, layer_idx, strength):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.steering_method = steering_method\n        self.layer_idx = layer_idx\n        self.strength = strength\n        self.hooks = []\n        \n    def add_steering_hook(self, direction=\"positive\"):\n        def steering_hook(module, input, output):\n            hidden_states = output[0]\n            # Apply steering to last token\n            last_token = hidden_states[:, -1:, :]\n            if hasattr(self.steering_method, 'apply_steering'):\n                if direction == \"negative\" and hasattr(self.steering_method, 'get_bidirectional_vectors'):\n                    steered = self.steering_method.apply_steering(last_token, self.strength, direction=\"negative\")\n                else:\n                    steered = self.steering_method.apply_steering(last_token, self.strength)\n                hidden_states[:, -1:, :] = steered\n            return (hidden_states,) + output[1:]\n        \n        handle = self.model.model.layers[self.layer_idx].register_forward_hook(steering_hook)\n        self.hooks.append(handle)\n        \n    def remove_hooks(self):\n        for handle in self.hooks:\n            handle.remove()\n        self.hooks = []\n        \n    def generate(self, prompt, direction=\"positive\"):\n        self.add_steering_hook(direction)\n        \n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=30).to(device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_length=inputs['input_ids'].shape[1] + MAX_LENGTH,\n                do_sample=True,\n                temperature=0.7,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n        \n        self.remove_hooks()\n        \n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return response[len(prompt):].strip()\n\ndef generate_unsteered(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=30).to(device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=inputs['input_ids'].shape[1] + MAX_LENGTH,\n            do_sample=True,\n            temperature=0.7,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response[len(prompt):].strip()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate and display responses\nfor prompt in TEST_PROMPTS:\n    print(f\"Prompt: {prompt}\")\n    print()\n    \n    # Unsteered\n    unsteered = generate_unsteered(prompt)\n    print(f\"Unsteered: {unsteered}\")\n    \n    # DAC\n    dac_wrapper = SteeringModelWrapper(model, tokenizer, dac, LAYER_INDEX, STEERING_STRENGTH)\n    dac_response = dac_wrapper.generate(prompt)\n    print(f\"DAC: {dac_response}\")\n    \n    # CAA\n    caa_wrapper = SteeringModelWrapper(model, tokenizer, caa, LAYER_INDEX, STEERING_STRENGTH)\n    caa_response = caa_wrapper.generate(prompt)\n    print(f\"CAA: {caa_response}\")\n    \n    # HPR\n    hpr_wrapper = SteeringModelWrapper(model, tokenizer, hpr, LAYER_INDEX, STEERING_STRENGTH)\n    hpr_response = hpr_wrapper.generate(prompt)\n    print(f\"HPR: {hpr_response}\")\n    \n    # BiPO Positive\n    bipo_wrapper = SteeringModelWrapper(model, tokenizer, bipo, LAYER_INDEX, STEERING_STRENGTH)\n    bipo_pos_response = bipo_wrapper.generate(prompt, \"positive\")\n    print(f\"BiPO Positive: {bipo_pos_response}\")\n    \n    # BiPO Negative\n    bipo_neg_response = bipo_wrapper.generate(prompt, \"negative\")\n    print(f\"BiPO Negative: {bipo_neg_response}\")\n    \n    # K-Steering\n    k_wrapper = SteeringModelWrapper(model, tokenizer, k_steering, LAYER_INDEX, STEERING_STRENGTH)\n    k_response = k_wrapper.generate(prompt)\n    print(f\"K-Steering: {k_response}\")\n    \n    print()\n    print(\"---\")\n    print()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}