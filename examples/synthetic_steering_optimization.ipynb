{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comprehensive Steering Parameter Optimization\n\nThis notebook demonstrates optimization of all steering parameters, matching what the Wisent Guard CLI does.\nWe'll optimize:\n- Steering strength/alpha\n- Layer selection\n- Method-specific parameters (normalization, beta values, epochs, etc.)\n- Multiple methods and their variations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport torch\nimport numpy as np\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List, Tuple, Any\nfrom dataclasses import dataclass\n\n# Add project root to path\nproject_root = Path.cwd().parent\nsys.path.insert(0, str(project_root))\n\nfrom wisent_guard.core.steering_methods.dac import DAC\nfrom wisent_guard.core.steering_methods.caa import CAA, ControlVectorAggregationMethod\nfrom wisent_guard.core.steering_methods.hpr import HPR\nfrom wisent_guard.core.steering_methods.bipo import BiPO\nfrom wisent_guard.core.steering_methods.k_steering import KSteering\nfrom wisent_guard.core.contrastive_pairs.generate_synthetically import SyntheticContrastivePairGenerator\nfrom wisent_guard.core.contrastive_pairs import ContrastivePairSet\nfrom wisent_guard.core.model import Model\nfrom evaluate_personal import SteeringEvaluator\n\n# Parameters\nMODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\nBASE_LAYER = 15\n# Search over ALL layers (0-31 for Llama 3.1 8B)\nLAYER_RANGE = list(range(0, 32))  # All 32 layers\nMAX_LENGTH = 30\nNUM_PAIRS = 30  # Generate 30 pairs as requested"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model and device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32).to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = Model(name=MODEL_NAME, hf_model=hf_model)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = SteeringEvaluator(hf_model, tokenizer, device)\n",
    "print(\"✓ Model and evaluator loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define traits and test prompts\nTRAITS = [\"evil\", \"sexual\", \"British\", \"helpful\", \"creative\"]\n\n# 5 unique test prompts that work for all traits\nTEST_PROMPTS = [\n    \"What should I do today?\",\n    \"Tell me about your thoughts on life.\",\n    \"How would you handle a difficult situation?\",\n    \"What's your opinion on modern technology?\",\n    \"Describe your ideal weekend.\"\n]\n\n# Select which trait to optimize (change index to test different traits)\nTRAIT_INDEX = 0  # 0=evil, 1=sexual, 2=British, 3=helpful, 4=creative\nTRAIT_NAME = TRAITS[TRAIT_INDEX]\n\nprint(f\"Optimizing for trait: {TRAIT_NAME}\")\nprint(f\"Test prompts: {TEST_PROMPTS}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass SteeringConfig:\n    \"\"\"Configuration for a steering method with all parameters.\"\"\"\n    name: str\n    method_class: type\n    init_params: Dict[str, Any]\n    steering_params: Dict[str, List[Any]]  # Parameters to optimize\n    \n# Define all steering configurations matching the CLI\nsteering_configs = [\n    # CAA variations\n    SteeringConfig(\n        name=\"CAA\",\n        method_class=CAA,\n        init_params={\"device\": device},\n        steering_params={\n            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n            \"layer\": LAYER_RANGE\n        }\n    ),\n    SteeringConfig(\n        name=\"CAA_L2\",\n        method_class=CAA,\n        init_params={\"device\": device, \"normalization_method\": \"l2_unit\"},\n        steering_params={\n            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n            \"layer\": LAYER_RANGE\n        }\n    ),\n    SteeringConfig(\n        name=\"CAA_CrossBehavior\",\n        method_class=CAA,\n        init_params={\"device\": device, \"normalization_method\": \"cross_behavior\"},\n        steering_params={\n            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n            \"layer\": LAYER_RANGE\n        }\n    ),\n    \n    # DAC variations\n    SteeringConfig(\n        name=\"DAC_Dynamic\",\n        method_class=DAC,\n        init_params={\"device\": device},\n        steering_params={\n            \"alpha\": [0.5, 1.0, 1.5, 2.0, 2.5, 3.0],  # DAC uses alpha not strength\n            \"layer\": LAYER_RANGE\n        }\n    ),\n    \n    # HPR (Harmonic Preference Regularization)\n    SteeringConfig(\n        name=\"HPR\",\n        method_class=HPR,\n        init_params={\n            \"device\": device,\n            \"learning_rate\": 1e-3,  # Default, will also optimize\n            \"epochs\": 100,  # Default, will also optimize\n            \"batch_size\": 32,\n            \"hidden_size\": 128,\n            \"angle_loss_weight\": 0.1  # Default, will also optimize\n        },\n        steering_params={\n            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n            \"layer\": LAYER_RANGE,\n            \"learning_rate\": [5e-4, 1e-3, 5e-3],  # Optimize learning rate\n            \"epochs\": [50, 100, 200],  # Optimize epochs\n            \"angle_loss_weight\": [0.05, 0.1, 0.2]  # Optimize angle loss weight\n        }\n    ),\n    \n    # BiPO (Bi-directional Preference Optimization)\n    SteeringConfig(\n        name=\"BiPO\",\n        method_class=BiPO,\n        init_params={\n            \"device\": device,\n            \"beta\": 0.1,  # Default, will also optimize\n            \"learning_rate\": 5e-4,  # Default, will also optimize\n            \"num_epochs\": 100,  # Default, will also optimize\n            \"batch_size\": 16,\n            \"reference_free\": True\n        },\n        steering_params={\n            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n            \"layer\": LAYER_RANGE,\n            \"beta\": [0.05, 0.1, 0.2, 0.5],  # Optimize DPO beta parameter\n            \"learning_rate\": [1e-4, 5e-4, 1e-3],  # Optimize learning rate\n            \"num_epochs\": [50, 100, 200]  # Optimize number of epochs\n        }\n    ),\n    \n    # BiPO with reference model\n    SteeringConfig(\n        name=\"BiPO_Reference\",\n        method_class=BiPO,\n        init_params={\n            \"device\": device,\n            \"beta\": 0.1,\n            \"learning_rate\": 5e-4,\n            \"num_epochs\": 100,\n            \"batch_size\": 16,\n            \"reference_free\": False  # Use reference model approach\n        },\n        steering_params={\n            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n            \"layer\": LAYER_RANGE,\n            \"beta\": [0.05, 0.1, 0.2, 0.5],\n            \"learning_rate\": [1e-4, 5e-4, 1e-3],\n            \"num_epochs\": [50, 100, 200]\n        }\n    ),\n]\n\nprint(f\"Defined {len(steering_configs)} steering configurations to test\")\nprint(f\"Will search over {len(LAYER_RANGE)} layers: {LAYER_RANGE}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Generate synthetic pairs once (not per layer)\nprint(\"\\nGenerating synthetic pairs...\")\ngenerator = SyntheticContrastivePairGenerator(model)\n\n# Generate a single set of pairs using the trait name as description\npair_set = generator.generate_contrastive_pair_set(\n    trait_description=TRAIT_NAME,\n    num_pairs=NUM_PAIRS,\n    name=TRAIT_NAME\n)\n\nprint(f\"✓ Generated {len(pair_set.pairs)} pairs\")\n\n# Now extract activations for each layer from the same pairs\nprint(\"\\nExtracting activations for all layers...\")\nlayer_pair_sets = {}\n\ndef extract_activations(text, layer_idx):\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    activations = []\n    def hook(module, input, output):\n        activations.append(output[0][:, -1, :].clone())\n    handle = hf_model.model.layers[layer_idx].register_forward_hook(hook)\n    with torch.no_grad():\n        hf_model(**inputs)\n    handle.remove()\n    return activations[0].squeeze(0)\n\n# For each layer, create a copy of the pair set with activations from that layer\nfor layer in LAYER_RANGE:\n    print(f\"Extracting activations for layer {layer}...\")\n    \n    # Create a deep copy of the pair set\n    layer_pair_set = ContrastivePairSet(\n        name=pair_set.name,\n        category=pair_set.category,\n        trait_description=pair_set.trait_description\n    )\n    \n    # Copy pairs with new activations for this layer\n    for pair in pair_set.pairs:\n        # Create a new pair with the same text but different activations\n        new_pair = type(pair)(\n            prompt=pair.prompt,\n            positive_response=type(pair.positive_response)(\n                text=pair.positive_response.text,\n                activations=extract_activations(pair.positive_response.text, layer)\n            ),\n            negative_response=type(pair.negative_response)(\n                text=pair.negative_response.text,\n                activations=extract_activations(pair.negative_response.text, layer)\n            )\n        )\n        layer_pair_set.add_pair(new_pair)\n    \n    layer_pair_sets[layer] = layer_pair_set\n\nprint(\"✓ Activations extracted for all layers\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generation functions with proper parameter handling\ndef generate_unsteered(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = hf_model.generate(\n            **inputs,\n            max_length=inputs[\"input_ids\"].shape[1] + MAX_LENGTH,\n            do_sample=True,\n            temperature=0.7,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response[len(prompt):].strip()\n\ndef generate_with_steering(prompt, steering_method, layer, **params):\n    \"\"\"Generate with steering, handling different parameter names for different methods.\"\"\"\n    # Get the right parameter name for this method\n    if hasattr(steering_method, 'apply_steering'):\n        # DAC uses 'alpha' parameter\n        if isinstance(steering_method, DAC) and 'strength' in params:\n            params['alpha'] = params.pop('strength')\n        # K-Steering might have its own alpha parameter separate from strength\n        elif isinstance(steering_method, KSteering) and 'alpha' in params:\n            # K-Steering alpha is set during training, not during application\n            params.pop('alpha', None)\n    \n    def steering_hook(module, input, output):\n        hidden_states = output[0]\n        last_token = hidden_states[:, -1:, :]\n        # Apply steering with the appropriate parameters\n        if isinstance(steering_method, DAC):\n            steered = steering_method.apply_steering(last_token, strength=params.get('alpha', 1.0))\n        else:\n            steered = steering_method.apply_steering(last_token, strength=params.get('strength', 1.0))\n        hidden_states[:, -1:, :] = steered\n        return (hidden_states,) + output[1:]\n    \n    handle = hf_model.model.layers[layer].register_forward_hook(steering_hook)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = hf_model.generate(\n            **inputs,\n            max_length=inputs[\"input_ids\"].shape[1] + MAX_LENGTH,\n            do_sample=True,\n            temperature=0.7,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    handle.remove()\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response[len(prompt):].strip()\n\n# Generate unsteered baselines for all test prompts\nprint(\"Generating unsteered responses for all test prompts...\")\nunsteered_responses = {}\nfor prompt in TEST_PROMPTS:\n    unsteered_responses[prompt] = generate_unsteered(prompt)\n    print(f\"Prompt: {prompt}\")\n    print(f\"Response: {unsteered_responses[prompt]}\\n\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Comprehensive Parameter Optimization\n\nNow let's optimize all parameters for each steering method configuration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Function to optimize all parameters for a steering configuration\nfrom itertools import product\n\ndef optimize_steering_config(config: SteeringConfig, max_combinations: int = 50):\n    \"\"\"Optimize all parameters for a steering configuration.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"OPTIMIZING {config.name}\")\n    print(f\"{'='*60}\")\n    \n    best_score = -1\n    best_params = {}\n    best_responses = {}\n    all_results = []\n    \n    # Generate all parameter combinations\n    param_names = list(config.steering_params.keys())\n    param_values = list(config.steering_params.values())\n    all_combinations = list(product(*param_values))\n    \n    # Limit combinations if too many\n    if len(all_combinations) > max_combinations:\n        # Sample randomly\n        import random\n        random.shuffle(all_combinations)\n        all_combinations = all_combinations[:max_combinations]\n    \n    print(f\"Testing {len(all_combinations)} parameter combinations...\")\n    \n    for i, param_tuple in enumerate(all_combinations):\n        params = dict(zip(param_names, param_tuple))\n        layer = params.pop('layer')  # Layer is special, not passed to method\n        \n        try:\n            # Initialize method with the right parameters\n            init_params = config.init_params.copy()\n            \n            # Handle training parameters that go into init for HPR\n            if config.name.startswith(\"HPR\"):\n                if 'learning_rate' in params:\n                    init_params['learning_rate'] = params.pop('learning_rate')\n                if 'epochs' in params:\n                    init_params['epochs'] = params.pop('epochs')\n                if 'angle_loss_weight' in params:\n                    init_params['angle_loss_weight'] = params.pop('angle_loss_weight')\n            \n            # Handle training parameters that go into init for BiPO\n            if config.name.startswith(\"BiPO\"):\n                if 'beta' in params:\n                    init_params['beta'] = params.pop('beta')\n                if 'learning_rate' in params:\n                    init_params['learning_rate'] = params.pop('learning_rate')\n                if 'num_epochs' in params:\n                    init_params['num_epochs'] = params.pop('num_epochs')\n            \n            # Create and train the method\n            method = config.method_class(**init_params)\n            \n            # Special handling for different methods\n            if isinstance(method, DAC):\n                method.set_model_reference(hf_model)\n            \n            # Train on the appropriate layer's data\n            if layer in layer_pair_sets:\n                method.train(layer_pair_sets[layer], layer)\n            else:\n                print(f\"Warning: No training data for layer {layer}\")\n                continue\n            \n            # Generate steered responses and evaluate on all test prompts\n            prompt_scores = []\n            prompt_responses = {}\n            \n            for prompt in TEST_PROMPTS:\n                steered_response = generate_with_steering(prompt, method, layer, **params)\n                prompt_responses[prompt] = steered_response\n                \n                # Evaluate\n                scores = evaluator.evaluate_response(\n                    prompt, unsteered_responses[prompt], steered_response, TRAIT_NAME\n                )\n                prompt_scores.append(scores['overall'])\n            \n            # Average score across all prompts\n            avg_score = sum(prompt_scores) / len(prompt_scores)\n            \n            result = {\n                'params': {**params, 'layer': layer, **{k: v for k, v in init_params.items() if k != 'device'}},\n                'avg_score': avg_score,\n                'prompt_scores': prompt_scores,\n                'responses': prompt_responses\n            }\n            all_results.append(result)\n            \n            # Update best if needed\n            if avg_score > best_score:\n                best_score = avg_score\n                best_params = result['params']\n                best_responses = prompt_responses\n            \n            # Progress update\n            if (i + 1) % 10 == 0:\n                print(f\"  Progress: {i+1}/{len(all_combinations)} - Best avg score so far: {best_score:.1f}\")\n                \n        except Exception as e:\n            print(f\"  Error with params {params}: {str(e)[:100]}\")\n            continue\n    \n    print(f\"\\nBest parameters for {config.name}:\")\n    for param, value in best_params.items():\n        print(f\"  {param}: {value}\")\n    print(f\"Best average score: {best_score:.1f}/10\")\n    print(f\"\\nBest responses for each prompt:\")\n    for prompt, response in best_responses.items():\n        print(f\"  {prompt}: {response[:50]}...\")\n    \n    return {\n        'config_name': config.name,\n        'best_params': best_params,\n        'best_score': best_score,\n        'best_responses': best_responses,\n        'all_results': all_results\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optimize a subset of configurations (for demonstration)\n# In practice, you'd run all of them, but it takes time\nconfigs_to_test = [\n    steering_configs[0],  # CAA\n    steering_configs[1],  # CAA_L2\n    steering_configs[3],  # DAC_Dynamic\n]\n\noptimization_results = []\nfor config in configs_to_test:\n    try:\n        result = optimize_steering_config(config)\n        optimization_results.append(result)\n    except Exception as e:\n        print(f\"Failed to optimize {config.name}: {e}\")\n\n<parameter name=\"cell_type\">code"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Optimize a subset of configurations (for demonstration)\n# Test the new HPR and BiPO methods along with existing ones\nconfigs_to_test = [\n    steering_configs[0],  # CAA\n    steering_configs[3],  # DAC_Dynamic\n    steering_configs[4],  # HPR\n    steering_configs[5],  # BiPO\n]\n\noptimization_results = []\nfor config in configs_to_test:\n    try:\n        result = optimize_steering_config(config, max_combinations=30)  # Limited for speed\n        optimization_results.append(result)\n    except Exception as e:\n        print(f\"Failed to optimize {config.name}: {e}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze which layers work best across methods\nlayer_effectiveness = {}\nfor layer in LAYER_RANGE:\n    layer_effectiveness[layer] = []\n\nfor result in optimization_results:\n    for test_result in result['all_results']:\n        layer = test_result['params']['layer']\n        score = test_result['avg_score']\n        layer_effectiveness[layer].append(score)\n\n# Calculate average effectiveness per layer\nprint(\"\\nLayer Effectiveness Analysis:\")\nprint(\"=\"*40)\nfor layer, scores in layer_effectiveness.items():\n    if scores:\n        avg_score = sum(scores) / len(scores)\n        print(f\"Layer {layer}: {avg_score:.2f} (based on {len(scores)} tests)\")\n\n# Plot layer effectiveness\nplt.figure(figsize=(10, 6))\nlayers = list(layer_effectiveness.keys())\navg_scores = [sum(scores)/len(scores) if scores else 0 for scores in layer_effectiveness.values()]\nplt.bar(layers, avg_scores)\nplt.xlabel('Layer')\nplt.ylabel('Average Effectiveness Score')\nplt.title(f'Steering Effectiveness by Layer for \"{TRAIT_NAME}\" trait')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Parameter Sensitivity Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze parameter sensitivity for each method\ndef analyze_parameter_sensitivity(optimization_result):\n    \"\"\"Analyze how sensitive the score is to each parameter.\"\"\"\n    config_name = optimization_result['config_name']\n    all_results = optimization_result['all_results']\n    \n    if not all_results:\n        return\n    \n    print(f\"\\nParameter Sensitivity for {config_name}:\")\n    print(\"-\" * 40)\n    \n    # Get all parameter names (excluding layer for now)\n    param_names = [k for k in all_results[0]['params'].keys() if k != 'layer']\n    \n    for param_name in param_names:\n        # Group results by parameter value\n        param_groups = {}\n        for result in all_results:\n            param_val = result['params'].get(param_name)\n            if param_val is not None:\n                if param_val not in param_groups:\n                    param_groups[param_val] = []\n                param_groups[param_val].append(result['avg_score'])\n        \n        # Calculate variance across parameter values\n        if len(param_groups) > 1:\n            group_means = {val: sum(scores)/len(scores) for val, scores in param_groups.items()}\n            variance = np.var(list(group_means.values()))\n            \n            print(f\"{param_name}:\")\n            for val, mean_score in sorted(group_means.items()):\n                print(f\"  {val}: {mean_score:.2f}\")\n            print(f\"  Variance: {variance:.3f}\")\n\n# Run sensitivity analysis\nfor result in optimization_results:\n    analyze_parameter_sensitivity(result)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison\n",
    "\n",
    "Let's compare the optimal parameters and performance across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final comparison of all optimized methods\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL COMPARISON - OPTIMAL CONFIGURATIONS\")\nprint(\"=\"*60)\n\n# Sort methods by best score\nsorted_results = sorted(optimization_results, key=lambda x: x['best_score'], reverse=True)\n\nprint(f\"\\nMethod Ranking by Overall Score for '{TRAIT_NAME}' trait:\")\nprint(\"-\" * 40)\nfor i, result in enumerate(sorted_results, 1):\n    print(f\"\\n{i}. {result['config_name']}:\")\n    print(f\"   Best average score: {result['best_score']:.1f}/10\")\n    print(f\"   Optimal parameters:\")\n    for param, value in result['best_params'].items():\n        print(f\"     - {param}: {value}\")\n    print(f\"   Example responses:\")\n    for j, (prompt, response) in enumerate(list(result['best_responses'].items())[:2]):\n        print(f\"     Prompt {j+1}: {prompt}\")\n        print(f\"     Response: {response[:60]}...\")\n\n# Create summary table\nprint(\"\\n\\nSummary Table:\")\nprint(\"-\" * 80)\nprint(f\"{'Method':<20} {'Avg Score':<12} {'Layer':<10} {'Strength/Alpha':<15} {'Other Params':<20}\")\nprint(\"-\" * 80)\nfor result in sorted_results:\n    method = result['config_name']\n    score = result['best_score']\n    layer = result['best_params'].get('layer', 'N/A')\n    strength = result['best_params'].get('strength', result['best_params'].get('alpha', 'N/A'))\n    \n    # Collect other important params\n    other_params = []\n    for k, v in result['best_params'].items():\n        if k not in ['layer', 'strength', 'alpha']:\n            other_params.append(f\"{k}={v}\")\n    other_str = ', '.join(other_params[:2]) if other_params else 'N/A'\n    \n    print(f\"{method:<20} {score:<12.1f} {layer:<10} {strength:<15.2f} {other_str:<20}\")\n\nprint(\"\\n✅ Comprehensive optimization complete!\")\nprint(\"\\nKey Insights:\")\nprint(\"1. Different normalization methods (L2, cross-behavior) can significantly affect performance\")\nprint(\"2. Optimal layer varies by method - not always the classification layer\")\nprint(\"3. Strength/alpha parameters need careful tuning for each method\")\nprint(\"4. HPR and BiPO have additional training hyperparameters that affect performance\")\nprint(\"5. Evaluation across multiple prompts provides more robust optimization\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}