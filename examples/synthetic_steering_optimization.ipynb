{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comprehensive Steering Parameter Optimization\n\nThis notebook demonstrates optimization of all steering parameters, matching what the Wisent Guard CLI does.\nWe'll optimize:\n- Steering strength/alpha\n- Layer selection\n- Method-specific parameters (normalization, beta values, epochs, etc.)\n- Multiple methods and their variations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport torch\nimport numpy as np\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List, Tuple, Any\nfrom dataclasses import dataclass\n\n# Add project root to path\nproject_root = Path.cwd().parent\nsys.path.insert(0, str(project_root))\n\nfrom wisent_guard.core.steering_methods.dac import DAC\nfrom wisent_guard.core.steering_methods.caa import CAA, ControlVectorAggregationMethod\nfrom wisent_guard.core.steering_methods.hpr import HPR\nfrom wisent_guard.core.steering_methods.bipo import BiPO\nfrom wisent_guard.core.steering_methods.k_steering import KSteering\nfrom wisent_guard.core.contrastive_pairs.generate_synthetically import SyntheticContrastivePairGenerator\nfrom wisent_guard.core.model import Model\nfrom evaluate_personal import SteeringEvaluator\n\n# Parameters\nMODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\nBASE_LAYER = 15\nLAYER_RANGE = [13, 14, 15, 16, 17]  # Layers to test\nMAX_LENGTH = 30\nNUM_PAIRS = 5  # More pairs for better training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model and device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32).to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = Model(name=MODEL_NAME, hf_model=hf_model)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = SteeringEvaluator(hf_model, tokenizer, device)\n",
    "print(\"✓ Model and evaluator loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trait and test prompt\n",
    "TRAIT_NAME = \"sarcastic\"\n",
    "TRAIT_DESCRIPTION = \"sarcastic and witty responses with subtle mockery and irony\"\n",
    "TEST_PROMPT = \"What's your opinion on Monday mornings?\"\n",
    "\n",
    "print(f\"Trait: {TRAIT_NAME}\")\n",
    "print(f\"Description: {TRAIT_DESCRIPTION}\")\n",
    "print(f\"Test prompt: {TEST_PROMPT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic pairs and train steering methods\n",
    "print(\"\\nGenerating synthetic pairs...\")\n",
    "generator = SyntheticContrastivePairGenerator(model)\n",
    "pair_set = generator.generate_contrastive_pair_set(\n",
    "    trait_description=TRAIT_DESCRIPTION,\n",
    "    num_pairs=NUM_PAIRS,\n",
    "    name=TRAIT_NAME\n",
    ")\n",
    "\n",
    "# Extract activations\n",
    "def extract_activations(text, layer_idx):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    activations = []\n",
    "    def hook(module, input, output):\n",
    "        activations.append(output[0][:, -1, :].clone())\n",
    "    handle = hf_model.model.layers[layer_idx].register_forward_hook(hook)\n",
    "    with torch.no_grad():\n",
    "        hf_model(**inputs)\n",
    "    handle.remove()\n",
    "    return activations[0].squeeze(0)\n",
    "\n",
    "for pair in pair_set.pairs:\n",
    "    pair.positive_response.activations = extract_activations(pair.positive_response.text, LAYER_INDEX)\n",
    "    pair.negative_response.activations = extract_activations(pair.negative_response.text, LAYER_INDEX)\n",
    "\n",
    "# Train methods\n",
    "print(\"Training steering methods...\")\n",
    "dac = DAC(device=device)\n",
    "dac.set_model_reference(hf_model)\n",
    "dac.train(pair_set, LAYER_INDEX)\n",
    "\n",
    "caa = CAA(device=device)\n",
    "caa.train(pair_set, LAYER_INDEX)\n",
    "\n",
    "print(\"✓ Methods trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation functions\n",
    "def generate_unsteered(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = hf_model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + MAX_LENGTH,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "def generate_with_steering(prompt, steering_method, strength):\n",
    "    def steering_hook(module, input, output):\n",
    "        hidden_states = output[0]\n",
    "        last_token = hidden_states[:, -1:, :]\n",
    "        steered = steering_method.apply_steering(last_token, strength)\n",
    "        hidden_states[:, -1:, :] = steered\n",
    "        return (hidden_states,) + output[1:]\n",
    "    \n",
    "    handle = hf_model.model.layers[LAYER_INDEX].register_forward_hook(steering_hook)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = hf_model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + MAX_LENGTH,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    handle.remove()\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "# Generate unsteered baseline\n",
    "unsteered_response = generate_unsteered(TEST_PROMPT)\n",
    "print(f\"Unsteered response: {unsteered_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Generate synthetic pairs for each layer we'll test\nprint(\"\\nGenerating synthetic pairs for each layer...\")\ngenerator = SyntheticContrastivePairGenerator(model)\n\n# Store pair sets for each layer\nlayer_pair_sets = {}\nfor layer in LAYER_RANGE:\n    print(f\"Generating pairs for layer {layer}...\")\n    pair_set = generator.generate_contrastive_pair_set(\n        trait_description=TRAIT_DESCRIPTION,\n        num_pairs=NUM_PAIRS,\n        name=TRAIT_NAME\n    )\n    \n    # Extract activations for this layer\n    def extract_activations(text, layer_idx):\n        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n        activations = []\n        def hook(module, input, output):\n            activations.append(output[0][:, -1, :].clone())\n        handle = hf_model.model.layers[layer_idx].register_forward_hook(hook)\n        with torch.no_grad():\n            hf_model(**inputs)\n        handle.remove()\n        return activations[0].squeeze(0)\n    \n    for pair in pair_set.pairs:\n        pair.positive_response.activations = extract_activations(pair.positive_response.text, layer)\n        pair.negative_response.activations = extract_activations(pair.negative_response.text, layer)\n    \n    layer_pair_sets[layer] = pair_set\n\nprint(\"✓ Pairs generated for all layers\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generation functions with proper parameter handling\ndef generate_unsteered(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = hf_model.generate(\n            **inputs,\n            max_length=inputs[\"input_ids\"].shape[1] + MAX_LENGTH,\n            do_sample=True,\n            temperature=0.7,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response[len(prompt):].strip()\n\ndef generate_with_steering(prompt, steering_method, layer, **params):\n    \"\"\"Generate with steering, handling different parameter names for different methods.\"\"\"\n    # Get the right parameter name for this method\n    if hasattr(steering_method, 'apply_steering'):\n        # DAC uses 'alpha' parameter\n        if isinstance(steering_method, DAC) and 'strength' in params:\n            params['alpha'] = params.pop('strength')\n        # K-Steering might have its own alpha parameter separate from strength\n        elif isinstance(steering_method, KSteering) and 'alpha' in params:\n            # K-Steering alpha is set during training, not during application\n            params.pop('alpha', None)\n    \n    def steering_hook(module, input, output):\n        hidden_states = output[0]\n        last_token = hidden_states[:, -1:, :]\n        # Apply steering with the appropriate parameters\n        if isinstance(steering_method, DAC):\n            steered = steering_method.apply_steering(last_token, strength=params.get('alpha', 1.0))\n        else:\n            steered = steering_method.apply_steering(last_token, strength=params.get('strength', 1.0))\n        hidden_states[:, -1:, :] = steered\n        return (hidden_states,) + output[1:]\n    \n    handle = hf_model.model.layers[layer].register_forward_hook(steering_hook)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = hf_model.generate(\n            **inputs,\n            max_length=inputs[\"input_ids\"].shape[1] + MAX_LENGTH,\n            do_sample=True,\n            temperature=0.7,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    handle.remove()\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response[len(prompt):].strip()\n\n# Generate unsteered baseline\nunsteered_response = generate_unsteered(TEST_PROMPT)\nprint(f\"Unsteered response: {unsteered_response}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Comprehensive Parameter Optimization\n\nNow let's optimize all parameters for each steering method configuration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Function to optimize all parameters for a steering configuration\nfrom itertools import product\n\ndef optimize_steering_config(config: SteeringConfig, max_combinations: int = 50):\n    \"\"\"Optimize all parameters for a steering configuration.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"OPTIMIZING {config.name}\")\n    print(f\"{'='*60}\")\n    \n    best_score = -1\n    best_params = {}\n    best_response = \"\"\n    all_results = []\n    \n    # Generate all parameter combinations\n    param_names = list(config.steering_params.keys())\n    param_values = list(config.steering_params.values())\n    all_combinations = list(product(*param_values))\n    \n    # Limit combinations if too many\n    if len(all_combinations) > max_combinations:\n        # Sample randomly\n        import random\n        random.shuffle(all_combinations)\n        all_combinations = all_combinations[:max_combinations]\n    \n    print(f\"Testing {len(all_combinations)} parameter combinations...\")\n    \n    for i, param_tuple in enumerate(all_combinations):\n        params = dict(zip(param_names, param_tuple))\n        layer = params.pop('layer')  # Layer is special, not passed to method\n        \n        try:\n            # Initialize method with the right parameters\n            init_params = config.init_params.copy()\n            \n            # Handle training parameters that go into init\n            if config.name.startswith(\"BiPO\") and 'learning_rate' in params:\n                init_params['learning_rate'] = params.pop('learning_rate')\n            if config.name.startswith(\"BiPO\") and 'epochs' in params:\n                init_params['epochs'] = params.pop('epochs')\n            \n            # Create and train the method\n            method = config.method_class(**init_params)\n            \n            # Special handling for different methods\n            if isinstance(method, DAC):\n                method.set_model_reference(hf_model)\n            \n            # Train on the appropriate layer's data\n            if layer in layer_pair_sets:\n                method.train(layer_pair_sets[layer], layer)\n            else:\n                print(f\"Warning: No training data for layer {layer}\")\n                continue\n            \n            # Generate steered response\n            steered_response = generate_with_steering(TEST_PROMPT, method, layer, **params)\n            \n            # Evaluate\n            scores = evaluator.evaluate_response(\n                TEST_PROMPT, unsteered_response, steered_response, TRAIT_DESCRIPTION\n            )\n            \n            result = {\n                'params': {**params, 'layer': layer},\n                'scores': scores,\n                'response': steered_response\n            }\n            all_results.append(result)\n            \n            # Update best if needed\n            if scores['overall'] > best_score:\n                best_score = scores['overall']\n                best_params = result['params']\n                best_response = steered_response\n            \n            # Progress update\n            if (i + 1) % 10 == 0:\n                print(f\"  Progress: {i+1}/{len(all_combinations)} - Best score so far: {best_score:.1f}\")\n                \n        except Exception as e:\n            print(f\"  Error with params {params}: {str(e)[:100]}\")\n            continue\n    \n    print(f\"\\nBest parameters for {config.name}:\")\n    for param, value in best_params.items():\n        print(f\"  {param}: {value}\")\n    print(f\"Best overall score: {best_score:.1f}/10\")\n    print(f\"Best response: {best_response}\")\n    \n    return {\n        'config_name': config.name,\n        'best_params': best_params,\n        'best_score': best_score,\n        'best_response': best_response,\n        'all_results': all_results\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optimize a subset of configurations (for demonstration)\n# In practice, you'd run all of them, but it takes time\nconfigs_to_test = [\n    steering_configs[0],  # CAA\n    steering_configs[1],  # CAA_L2\n    steering_configs[3],  # DAC_Dynamic\n]\n\noptimization_results = []\nfor config in configs_to_test:\n    try:\n        result = optimize_steering_config(config)\n        optimization_results.append(result)\n    except Exception as e:\n        print(f\"Failed to optimize {config.name}: {e}\")\n\n<parameter name=\"cell_type\">code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Analyzing Layer Effectiveness"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze which layers work best across methods\nlayer_effectiveness = {}\nfor layer in LAYER_RANGE:\n    layer_effectiveness[layer] = []\n\nfor result in optimization_results:\n    for test_result in result['all_results']:\n        layer = test_result['params']['layer']\n        score = test_result['scores']['overall']\n        layer_effectiveness[layer].append(score)\n\n# Calculate average effectiveness per layer\nprint(\"\\nLayer Effectiveness Analysis:\")\nprint(\"=\"*40)\nfor layer, scores in layer_effectiveness.items():\n    if scores:\n        avg_score = sum(scores) / len(scores)\n        print(f\"Layer {layer}: {avg_score:.2f} (based on {len(scores)} tests)\")\n\n# Plot layer effectiveness\nplt.figure(figsize=(10, 6))\nlayers = list(layer_effectiveness.keys())\navg_scores = [sum(scores)/len(scores) if scores else 0 for scores in layer_effectiveness.values()]\nplt.bar(layers, avg_scores)\nplt.xlabel('Layer')\nplt.ylabel('Average Effectiveness Score')\nplt.title('Steering Effectiveness by Layer')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Parameter Sensitivity Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze parameter sensitivity for each method\ndef analyze_parameter_sensitivity(optimization_result):\n    \"\"\"Analyze how sensitive the score is to each parameter.\"\"\"\n    config_name = optimization_result['config_name']\n    all_results = optimization_result['all_results']\n    \n    if not all_results:\n        return\n    \n    print(f\"\\nParameter Sensitivity for {config_name}:\")\n    print(\"-\" * 40)\n    \n    # Get all parameter names (excluding layer for now)\n    param_names = [k for k in all_results[0]['params'].keys() if k != 'layer']\n    \n    for param_name in param_names:\n        # Group results by parameter value\n        param_groups = {}\n        for result in all_results:\n            param_val = result['params'].get(param_name)\n            if param_val is not None:\n                if param_val not in param_groups:\n                    param_groups[param_val] = []\n                param_groups[param_val].append(result['scores']['overall'])\n        \n        # Calculate variance across parameter values\n        if len(param_groups) > 1:\n            group_means = {val: sum(scores)/len(scores) for val, scores in param_groups.items()}\n            variance = np.var(list(group_means.values()))\n            \n            print(f\"{param_name}:\")\n            for val, mean_score in sorted(group_means.items()):\n                print(f\"  {val}: {mean_score:.2f}\")\n            print(f\"  Variance: {variance:.3f}\")\n\n# Run sensitivity analysis\nfor result in optimization_results:\n    analyze_parameter_sensitivity(result)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison\n",
    "\n",
    "Let's compare the optimal parameters and performance across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final comparison of all optimized methods\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL COMPARISON - OPTIMAL CONFIGURATIONS\")\nprint(\"=\"*60)\n\n# Sort methods by best score\nsorted_results = sorted(optimization_results, key=lambda x: x['best_score'], reverse=True)\n\nprint(\"\\nMethod Ranking by Overall Score:\")\nprint(\"-\" * 40)\nfor i, result in enumerate(sorted_results, 1):\n    print(f\"\\n{i}. {result['config_name']}:\")\n    print(f\"   Best score: {result['best_score']:.1f}/10\")\n    print(f\"   Optimal parameters:\")\n    for param, value in result['best_params'].items():\n        print(f\"     - {param}: {value}\")\n    print(f\"   Example response: {result['best_response'][:100]}...\")\n\n# Create summary table\nprint(\"\\n\\nSummary Table:\")\nprint(\"-\" * 60)\nprint(f\"{'Method':<20} {'Score':<10} {'Layer':<10} {'Strength/Alpha':<15}\")\nprint(\"-\" * 60)\nfor result in sorted_results:\n    method = result['config_name']\n    score = result['best_score']\n    layer = result['best_params'].get('layer', 'N/A')\n    strength = result['best_params'].get('strength', result['best_params'].get('alpha', 'N/A'))\n    print(f\"{method:<20} {score:<10.1f} {layer:<10} {strength:<15.2f}\")\n\nprint(\"\\n✅ Comprehensive optimization complete!\")\nprint(\"\\nKey Insights:\")\nprint(\"1. Different normalization methods (L2, cross-behavior) can significantly affect performance\")\nprint(\"2. Optimal layer varies by method - not always the classification layer\")\nprint(\"3. Strength/alpha parameters need careful tuning for each method\")\nprint(\"4. Some methods work better for certain types of traits\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}